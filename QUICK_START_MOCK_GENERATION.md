# Mock Generation Pipeline - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ¯ 30ç§’å¿«é€Ÿä¸Šæ‰‹

### æœ€ç®€æ–¹å¼ï¼šä¸€è¡Œå‘½ä»¤æ‰§è¡Œ

```bash
cd d:\Pcode\LLM4VIS\llmscivis
python test/mock_generation/mock_generation.py --excel your_excel_file.xlsx
```

### æœ€å¿«æ–¹å¼ï¼š5è¡Œ Python ä»£ç 

```python
from test.mock_generation.mock_generation import MockGenerationPipeline

pipeline = MockGenerationPipeline('your_excel.xlsx')
result = pipeline.run_complete_pipeline()
print(f"å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
```

---

## ğŸ“‹ Excel æ–‡ä»¶æ ¼å¼

### æœ€å°å¿…éœ€åˆ—

| Benchmark prompt |
|------------------|
| render a cone    |
| draw a sphere    |

### å®Œæ•´å¯é€‰åˆ—

| Benchmark prompt | groundTruth | generatorPrompt | evaluatorPrompt |
|------------------|------------|-----------------|-----------------|
| render a cone | `<html>...</html>` | ï¼ˆå¯é€‰ï¼‰ | ï¼ˆå¯é€‰ï¼‰ |

---

## ğŸ”„ ä¸‰æ­¥å·¥ä½œæµ

### Step 1: æ‹“å±• â†’ æ£€ç´¢
```python
# è‡ªåŠ¨æ‰§è¡Œï¼šanalyze_query() â†’ RAGAgent.search()
# è¾“å‡ºï¼šanalysis_result, final_prompt, retrieval_results
```

### Step 2: ä»£ç ç”Ÿæˆ
```python
# è‡ªåŠ¨æ‰§è¡Œï¼šget_llm_response()
# è¾“å‡ºï¼šgenerated_code, generation_time
```

### Step 3: ä»£ç è¯„ä¼°
```python
# è‡ªåŠ¨æ‰§è¡Œï¼ševaluator_agent.evaluate()
# è¾“å‡ºï¼šscore, evaluation_result, evaluation_time
```

---

## ğŸ’¡ å¸¸è§ç”¨æ³•

### ç”¨æ³• 1ï¼šåŸºç¡€å¤„ç†

```python
from test.mock_generation.mock_generation import MockGenerationPipeline

pipeline = MockGenerationPipeline('data.xlsx')
result = pipeline.run_complete_pipeline()
```

### ç”¨æ³• 2ï¼šè‡ªå®šä¹‰æ¨¡å‹

```python
result = pipeline.run_complete_pipeline(
    generator='claude-sonnet-4',
    evaluator='deepseek-v3'
)
```

### ç”¨æ³• 3ï¼šä»…æ‰§è¡Œæ£€ç´¢

```python
from RAG.retriever_v3 import process_benchmark_prompts_for_generation

process_benchmark_prompts_for_generation('data.xlsx')
```

### ç”¨æ³• 4ï¼šæŸ¥çœ‹ç»“æœæ‘˜è¦

```python
pipeline = MockGenerationPipeline('data.xlsx')
result = pipeline.run_complete_pipeline()

if result['success']:
    pipeline.print_results_summary()
    print(f"å¤„ç†äº† {result['processed']} è¡Œï¼Œå¤±è´¥ {result['errors']} è¡Œ")
```

---

## ğŸ“Š è¾“å‡ºç¤ºä¾‹

### æ§åˆ¶å°è¾“å‡º

```
================================================================================
å¼€å§‹æ‰§è¡Œå®Œæ•´çš„ä»£ç ç”Ÿæˆå’Œè¯„ä¼°æµç¨‹
================================================================================

[Step 1] ä» Excel è¯»å–æ•°æ®å¹¶æ‰§è¡Œæ£€ç´¢...
[Processing] åŠ è½½ 5 è¡Œæ•°æ®

[Row 1/5] å¤„ç†: render a cone...
[Row 1] æˆåŠŸå¤„ç†ï¼Œè€—æ—¶: 0.45s

...

[Step 2] æ‰§è¡Œä»£ç ç”Ÿæˆå’Œè¯„ä¼°...
[Row 1/5] æ‰§è¡Œä»£ç ç”Ÿæˆå’Œè¯„ä¼°...
  - ç”Ÿæˆä»£ç ... âœ“ ä»£ç ç”ŸæˆæˆåŠŸ (2.3s)
  - è¯„ä¼°ä»£ç ... âœ“ è¯„ä¼°å®Œæˆ (1.2s)
  - ç»“æœ: åˆ†æ•°=8.5

...

================================================================================
å¤„ç†å®Œæˆç»Ÿè®¡
================================================================================
  - æ€»è¡Œæ•°: 5
  - æˆåŠŸå¤„ç†: 5
  - å¤±è´¥: 0
  - è¾“å‡ºæ–‡ä»¶: data_output.xlsx
```

### è¾“å‡º Excel æ–‡ä»¶

åŒ…å«æ‰€æœ‰åŸå§‹åˆ— + æ–°å¢åˆ—ï¼š
- `analysis_result` - æç¤ºè¯æ‹“å±•ç»“æœ
- `final_prompt` - æœ€ç»ˆæç¤ºè¯
- `retrieval_time` - æ£€ç´¢è€—æ—¶
- `generated_code` - ç”Ÿæˆçš„ä»£ç 
- `generation_time` - ç”Ÿæˆè€—æ—¶
- `score` - è¯„ä¼°åˆ†æ•°
- `evaluation_time` - è¯„ä¼°è€—æ—¶

---

## ğŸ”— ç›¸å…³å‘½ä»¤

### å‘½ä»¤è¡Œå‚æ•°

```bash
# å®Œæ•´æµç¨‹
python test/mock_generation/mock_generation.py \
    --excel input.xlsx \
    --generator deepseek-v3 \
    --evaluator deepseek-v3

# ä»…æ£€ç´¢
python -c "from RAG.retriever_v3 import process_benchmark_prompts_for_generation; process_benchmark_prompts_for_generation('input.xlsx')"

# éªŒè¯ä¾èµ–
python test_mock_generation_import.py
```

---

## ğŸš€ ä¸‰ç§è¿è¡Œæ–¹å¼å¯¹æ¯”

| æ–¹å¼ | å‘½ä»¤ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|
| **å‘½ä»¤è¡Œ** | `python test/mock_generation/mock_generation.py --excel ...` | å¿«é€Ÿæ‰¹å¤„ç† |
| **Python API** | `MockGenerationPipeline('file.xlsx').run_complete_pipeline()` | ä»£ç é›†æˆ |
| **åˆ†æ­¥æ‰§è¡Œ** | `process_benchmark_prompts_for_generation()` + è‡ªå®šä¹‰é€»è¾‘ | é«˜åº¦å®šåˆ¶ |

---

## âš¡ æ€§èƒ½å‚è€ƒ

| æ“ä½œ | è€—æ—¶ |
|------|------|
| å•è¡Œæ£€ç´¢ | ~0.3-0.5s |
| å•è¡Œç”Ÿæˆ | ~2-5s |
| å•è¡Œè¯„ä¼° | ~1-3s |
| **å•è¡Œæ€»è€—æ—¶** | **~3.3-8.5s** |
| **10è¡Œæ€»è€—æ—¶** | **~33-85s** |
| **100è¡Œæ€»è€—æ—¶** | **~5-14åˆ†é’Ÿ** |

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨ä½¿ç”¨å‰éªŒè¯ï¼š

- [ ] Excel æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å« "Benchmark prompt" åˆ—
- [ ] MongoDB æœåŠ¡å·²å¯åŠ¨ (localhost:27017)
- [ ] æ‰€éœ€çš„ Python åŒ…å·²å®‰è£… (pandas, openpyxl)
- [ ] æ¨¡å‹å·²åœ¨ `config/ollama_config.py` ä¸­é…ç½®
- [ ] æœ‰ç½‘ç»œè¿æ¥ï¼ˆå¦‚æœä½¿ç”¨è¿œç¨‹ LLMï¼‰

---

## ğŸ› å¸¸è§é—®é¢˜é€ŸæŸ¥

### Q: æ‰¾ä¸åˆ°æ¨¡å—é”™è¯¯

```
ModuleNotFoundError: No module named 'RAG'
```

**A**: ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œæˆ–æ·»åŠ è·¯å¾„ï¼š
```python
import sys
sys.path.insert(0, '/path/to/llmscivis')
```

### Q: MongoDB è¿æ¥å¤±è´¥

```
Connection refused: 27017
```

**A**: å¯åŠ¨ MongoDBï¼š
```bash
mongod
```

### Q: Excel åˆ—æ‰¾ä¸åˆ°

```
KeyError: 'Benchmark prompt'
```

**A**: æ£€æŸ¥ Excel æ–‡ä»¶æ˜¯å¦æœ‰ "Benchmark prompt" åˆ—ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰

### Q: ä»£ç ç”Ÿæˆè¶…æ—¶

**A**: å¢åŠ è¶…æ—¶æ—¶é—´æˆ–ä½¿ç”¨æ›´å¿«çš„æœ¬åœ°æ¨¡å‹

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

| æ–‡æ¡£ | ç”¨é€” |
|------|------|
| [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) | å®ç°ç»†èŠ‚ |
| [MOCK_GENERATION_INTEGRATION.md](MOCK_GENERATION_INTEGRATION.md) | é›†æˆæŒ‡å— |
| [test/mock_generation/README.md](test/mock_generation/README.md) | API å‚è€ƒ |
| [test/mock_generation/test_example.py](test/mock_generation/test_example.py) | ä»£ç ç¤ºä¾‹ |

---

## ğŸ“ å·¥ä½œæµå›¾

```
è¾“å…¥ Excel
  â†“
[MockGenerationPipeline]
  â”œâ”€â†’ [Step 1] æ£€ç´¢é˜¶æ®µ
  â”‚    â””â”€â†’ process_benchmark_prompts_for_generation()
  â”‚         â”œâ”€ analyze_query()    [æç¤ºè¯æ‹“å±•]
  â”‚         â””â”€ RAGAgent.search()   [RAG æ£€ç´¢]
  â”‚
  â”œâ”€â†’ [Step 2] ç”Ÿæˆé˜¶æ®µ
  â”‚    â””â”€â†’ get_llm_response()     [ä»£ç ç”Ÿæˆ]
  â”‚
  â””â”€â†’ [Step 3] è¯„ä¼°é˜¶æ®µ
       â””â”€â†’ evaluator_agent.evaluate()  [ä»£ç è¯„ä¼°]
  â†“
è¾“å‡º Excel (_output.xlsx)
```

---

## ğŸ’» å¿«é€Ÿç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€ä½¿ç”¨

```python
from test.mock_generation.mock_generation import MockGenerationPipeline

pipeline = MockGenerationPipeline('experiment_results/data.xlsx')
result = pipeline.run_complete_pipeline()

print(f"æˆåŠŸå¤„ç† {result['processed']} è¡Œ")
print(f"è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
```

### ç¤ºä¾‹ 2ï¼šæŸ¥çœ‹ç»“æœ

```python
# ... æ‰§è¡Œæµç¨‹ ...

# æ‰“å°æ‘˜è¦
pipeline.print_results_summary()

# æŸ¥çœ‹è¯¦ç»†ç»“æœ
for r in result['results']:
    print(f"è¡Œ {r['row_index']}: åˆ†æ•°={r['score']}, è€—æ—¶={r['generation_time']}s")
```

### ç¤ºä¾‹ 3ï¼šé”™è¯¯å¤„ç†

```python
result = pipeline.run_complete_pipeline()

if not result['success']:
    print(f"é”™è¯¯: {result['error']}")
else:
    print(f"æˆåŠŸ! å¤„ç†äº† {result['processed']}/{result['total_rows']} è¡Œ")
    if result['errors'] > 0:
        print(f"è­¦å‘Š: æœ‰ {result['errors']} è¡Œå¤„ç†å¤±è´¥")
```

---

## ğŸ” é…ç½®æŒ‡å—

### 1. é…ç½®æ¨¡å‹ (`config/ollama_config.py`)

```python
ollama_config.models_cst = {
    'deepseek-v3': '...',
    'claude-sonnet-4': '...',
}
```

### 2. é…ç½®æ•°æ®åº“ (`RAG/retriever_v3.py`)

```python
DB_HOST = 'localhost'
DB_PORT = 27017
DB_NAME = 'code_database'
```

---

**å°±è¿™ä¹ˆç®€å•ï¼ç°åœ¨ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨ Mock Generation Pipeline äº†ï¼** ğŸš€
