# Experimental Results Documentation

## Overview

This directory contains experimental results for VTK.js code generation comparing different LLM models with and without RAG (Retrieval-Augmented Generation) workflow.

## Directory Structure

The results are organized into **6 model directories**:

### Models with RAG Workflow (4 directories)

- `claude_sonnet_4/` - Claude Sonnet 4 with RAG
- `deepseek-v3/` - DeepSeek V3 with RAG
- `gpt_5_with_rag/` - GPT-5 with RAG
- `qwen3_max_with_rag/` - Qwen3 Max with RAG

### Models without RAG Workflow (2 directories)

- `gpt_5_without_rag/` - GPT-5 without RAG
- `qwen3_max_without_rag/` - Qwen3 Max without RAG

## Files in Each Task Directory

Each task subdirectory contains 5 files with experimental data and results:

### 1. `final_prompt.txt`

The complete prompt sent to the generation model, including:

- User requirements
- System instructions
- Retrieved VTK.js examples (if using RAG workflow)
- Generation guidelines

### 2. `generated_code.html`

The HTML/VTK.js code generated by the LLM model based on the final prompt.

### 3. `ground_truth.html`

The expected/reference implementation that represents the correct solution for the task.

### 4. `modified_code.html`

The generated code after modifications/corrections to match the expected result (if applicable).

### 5. `case_export_data.json`

Comprehensive metadata and configuration for this specific generation case, including:

- **Workflow configuration**: RAG settings, inquiry expansion status
- **Model information**: Generator and evaluator models used
- **Prompts**: Original prompt, generator prompt, evaluator prompt
- **Evaluation results**:
  - Multi-dimensional scores (Functionality, Visual Quality, Code Quality)
  - Overall score and critique
  - Detailed reasoning for each dimension
- **Retrieval results**: Retrieved examples with scores (if RAG enabled)
- **Pipeline analysis**: Step-by-step breakdown of required VTK modules
- **Metadata**: Timestamps, evaluation ID, execution time

## Evaluation Metrics

The evaluation in `case_export_data.json` assesses three dimensions:

1. **Functionality & Completeness**

   - Pipeline correctness
   - Required elements presence
   - Task completion accuracy
2. **Visual Fidelity**

   - Visual output similarity
   - Camera/rendering parameters
   - Color mapping accuracy
3. **Code Quality & Maintainability**

   - Code structure and readability
   - Best practices compliance
   - Variable naming clarity

**Overall Score**: Average of the three dimension scores
