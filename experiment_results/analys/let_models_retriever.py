# è®©æ¨¡å‹æ ¹æ®ä¸‹é¢çš„æç¤ºè¯å’Œè¯­æ–™åº“æ£€ç´¢å‡ºæœ€ç›¸å…³çš„è¯­æ–™ï¼Œå¹¶è®°å½•æ•´ä¸ªè¿‡ç¨‹éœ€è¦çš„æ—¶é—´ã€‚

import time
import os
import sys
import json
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
sys.path.insert(0, os.path.abspath(
    os.path.join(os.path.dirname(__file__), '..', '..')))

from llm_agent.ollma_chat import get_llm_response


tasks = [
    """Generate an HTML page using vtk.js to visualize the rotor dataset.
- Load the dataset from: http://127.0.0.1:5000/dataset/rotor.vti
- Set the active scalar array to "Pressure".
- Apply a slice along the Y axis at 95% depth of the dataset (convert percentage to slice index).
- Use a blue â†’ white â†’ red color map for pressure values, spanning from the minimum to maximum scalar range.
- Set opacity to fully opaque (no transparency variation).
- Add an orientation marker with XYZ axes in the bottom-right corner.
- No interactive GUI controls are required.""",
    """
 
Generate an HTML page using vtk.js to visualize the Deepwater dataset with isosurface rendering.

Load the dataset from: http://127.0.0.1:5000/dataset/deepwater.vti

Compute velocity magnitude from arrays v02 and v03; if not available, use prs as the scalar

Generate an isosurface at the mid-value of the scalar range

Use a blue â†’ white â†’ red color map spanning the scalar range (min to max)

Set the isosurface to fully opaque with smooth shading

Add an XYZ orientation marker in the bottom-right corner""",
    """

Generate an HTML page using vtk.js to visualize the Isabel dataset with streamline rendering.

Load the dataset from: http://127.0.0.1:5000/dataset/isabel.vti

Use the 'Velocity' array as the vector field for streamlines

Generate seed points at the center of the dataset with sufficient density to cover the domain

Compute streamlines following the velocity field

Render streamlines in cyan ([0, 1, 1]) with a specified line width

Render a dataset outline in red ([1, 0, 0]) with a specified line width""",
    """
Generate an HTML page using vtk.js to visualize the Redsea dataset with volume rendering.

Load the dataset from: http://127.0.0.1:5000/dataset/redsea.vti

Compute velocity magnitude from the 'velocity' array and set it as the active scalar

Apply volume rendering using a blue â†’ white â†’ red color map spanning the scalar range (min to max)

Apply a piecewise opacity function to control transparency across scalar values

Set shading, ambient, diffuse, and specular properties for realistic volume appearance

Adjust the camera to look along +Z and center on the dataset"""
]


def load_corpus(corpus_path: str) -> dict:
    """
    ä»æŒ‡å®šè·¯å¾„è¯»å–æ‰€æœ‰description.txtæ–‡ä»¶ï¼Œç»„æˆ(æ–‡ä»¶å¤¹å, descriptionå†…å®¹)çš„å­—å…¸

    Args:
        corpus_path: è¯­æ–™åº“æ–‡ä»¶å¤¹è·¯å¾„

    Returns:
        dict: {"folder_name": "description content", ...}
    """
    corpus = {}
    corpus_dir = Path(corpus_path)

    if not corpus_dir.exists():
        print(f"è­¦å‘Š: è¯­æ–™åº“è·¯å¾„ä¸å­˜åœ¨: {corpus_path}")
        return corpus

    # éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
    for folder in corpus_dir.iterdir():
        if folder.is_dir():
            description_file = folder / "description.txt"
            if description_file.exists():
                try:
                    with open(description_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        corpus[folder.name] = content
                        print(f"âœ“ å·²è¯»å–: {folder.name}")
                except Exception as e:
                    print(f"âœ— è¯»å–å¤±è´¥ {folder.name}: {e}")

    return corpus


def save_corpus_json(corpus: dict, output_path: str):
    """
    å°†è¯­æ–™åº“ä¿å­˜ä¸ºJSONæ–‡ä»¶

    Args:
        corpus: è¯­æ–™åº“å­—å…¸
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(corpus, f, ensure_ascii=False, indent=2)
        print(f"âœ“ è¯­æ–™åº“å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"âœ— ä¿å­˜è¯­æ–™åº“å¤±è´¥: {e}")


def create_retrieval_prompt(corpus: dict, query: str) -> str:
    """
    åˆ›å»ºç”¨äºæ¨¡å‹æ£€ç´¢çš„æç¤ºè¯ï¼Œè¦æ±‚è¿”å›ç»“æ„åŒ–JSON

    Args:
        corpus: è¯­æ–™åº“å­—å…¸
        query: ç”¨æˆ·æŸ¥è¯¢

    Returns:
        str: å®Œæ•´çš„æç¤ºè¯
    """
    corpus_text = "\n\n".join(
        [f"ã€{name}ã€‘\n{content}" for name, content in corpus.items()])

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªVTK.jså¯è§†åŒ–ä»£ç ç”ŸæˆåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ç”¨æˆ·éœ€æ±‚ï¼Œä»è¯­æ–™åº“ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„ç¤ºä¾‹æˆ–æ¨¡å¼ã€‚

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{query}

ã€è¯­æ–™åº“ã€‘
{corpus_text}

ã€ä»»åŠ¡ã€‘
1. ç†è§£ç”¨æˆ·çš„éœ€æ±‚æ„å›¾
2. ä»ä¸Šè¿°è¯­æ–™åº“ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„éƒ¨åˆ†ï¼ˆ3-6ä¸ªï¼‰
3. è¿”å›ç»“æ„åŒ–çš„JSONæ ¼å¼ç»“æœ

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»¥ä¸‹é¢çš„JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€markdownæˆ–ä»£ç å—æ ‡è®°ï¼š
{{
  "understanding": "ç”¨æˆ·éœ€æ±‚çš„ç®€çŸ­æ€»ç»“ï¼ˆä¸€å¥è¯ï¼‰",
  "retrieved_items": [
    {{
      "title": "Filter-ImageSlice",
      "relevance_score": 0.95,
      "reason": "ä¸ºä»€ä¹ˆè¿™é¡¹ç›¸å…³çš„ç®€çŸ­è¯´æ˜ï¼ˆä¸€å¥è¯ï¼‰"
    }},
    {{
      "title": "IO-HttpdatasetReader",
      "relevance_score": 0.90,
      "reason": "ä¸ºä»€ä¹ˆè¿™é¡¹ç›¸å…³çš„ç®€çŸ­è¯´æ˜ï¼ˆä¸€å¥è¯ï¼‰"
    }}
  ]
}}

é‡è¦ï¼šè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚"""

    return prompt


def parse_retrieval_response(response: str) -> dict:
    """
    è§£æLLMçš„ç»“æ„åŒ–JSONå“åº”
    
    Args:
        response: LLMè¿”å›çš„å“åº”æ–‡æœ¬
    
    Returns:
        dict: è§£æåçš„ç»“æ„åŒ–æ•°æ®
    """
    try:
        # å°è¯•ç›´æ¥è§£æJSON
        result = json.loads(response)
        return result
    except json.JSONDecodeError:
        # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
        try:
            # æŸ¥æ‰¾JSONçš„å¼€å§‹å’Œç»“æŸä½ç½®
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                return result
        except (json.JSONDecodeError, ValueError):
            pass
        
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›é”™è¯¯æ ‡è®°
        return {
            "understanding": "æ— æ³•è§£æå“åº”",
            "retrieved_items": [],
            "raw_response": response[:500]  # ä¿ç•™å‰500å­—ç¬¦ç”¨äºè°ƒè¯•
        }


def perform_retrieval(query: str, corpus: dict, model_name: str = "deepseek-v3") -> dict:
    """
    ä½¿ç”¨LLMè¿›è¡Œæ£€ç´¢

    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        corpus: è¯­æ–™åº“å­—å…¸
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°

    Returns:
        dict: åŒ…å«æŸ¥è¯¢ã€ç»“æœã€è€—æ—¶ç­‰ä¿¡æ¯
    """
    print(f"\nå¼€å§‹æ£€ç´¢ [{model_name}]: {query[:50]}...")

    start_time = time.time()

    try:
        prompt = create_retrieval_prompt(corpus, query)

        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„VTK.jså¯è§†åŒ–ä¸“å®¶ã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„å¯è§†åŒ–éœ€æ±‚
2. ä»ç°æœ‰ç¤ºä¾‹å’Œè¯­æ–™åº“ä¸­æ‰¾å‡ºæœ€ç›¸å…³çš„å†…å®¹
3. è¿”å›ç»“æ„åŒ–çš„JSONæ ¼å¼å“åº”
4. ä¸åŒ…å«ä»»ä½•markdownæˆ–ä»£ç å—æ ‡è®°"""

        response = get_llm_response(prompt, model_name, system_prompt)
        
        # è§£æç»“æ„åŒ–å“åº”
        parsed_response = parse_retrieval_response(response)

        elapsed_time = time.time() - start_time
        
        # æå–æ£€ç´¢åˆ°çš„æ¨¡å—åç§°åˆ—è¡¨
        retrieved_modules = []
        if "retrieved_items" in parsed_response:
            for item in parsed_response["retrieved_items"]:
                if isinstance(item, dict) and "title" in item:
                    retrieved_modules.append(item["title"])

        result = {
            "query": query,
            "model": model_name,
            "response": response,
            "parsed_response": parsed_response,
            "retrieved_modules": retrieved_modules,
            "elapsed_time": elapsed_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        print(f"âœ“ æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’ï¼Œæ£€ç´¢åˆ° {len(retrieved_modules)} ä¸ªæ¨¡å—")
        return result

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"âœ— æ£€ç´¢å¤±è´¥: {e}")

        return {
            "query": query,
            "model": model_name,
            "error": str(e),
            "elapsed_time": elapsed_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }


def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ£€ç´¢æµç¨‹
    """
    parser = argparse.ArgumentParser(description="LLMæ¨¡å‹æ£€ç´¢è¯­æ–™åº“")
    parser.add_argument("--corpus-path", type=str,
                        default="data/vtkjs-examples/prompt-sample",
                        help="è¯­æ–™åº“è·¯å¾„")
    parser.add_argument("--model", type=str, default="deepseek-v3",
                        help="ä½¿ç”¨çš„æ¨¡å‹")
    parser.add_argument("--output", type=str, default="retrieval_results_with_time.json",
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--corpus-output", type=str, default="corpus.json",
                        help="è¯­æ–™åº“JSONè¾“å‡ºè·¯å¾„")

    args = parser.parse_args()

    print("="*60)
    print("LLMæ¨¡å‹æ£€ç´¢ç³»ç»Ÿ - ç»“æ„åŒ–è¾“å‡ºç‰ˆæœ¬")
    print("="*60)

    # ç¬¬1æ­¥ï¼šåŠ è½½è¯­æ–™åº“
    print("\n[æ­¥éª¤1] åŠ è½½è¯­æ–™åº“...")
    corpus = load_corpus(args.corpus_path)

    if not corpus:
        print("âš  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯­æ–™åº“æ–‡ä»¶")
        print(f"  é¢„æœŸä½ç½®: {args.corpus_path}")
        # ä½¿ç”¨æ¼”ç¤ºè¯­æ–™åº“
        corpus = {
            "image_slice": "å¤„ç†2Då›¾åƒåˆ‡ç‰‡çš„ç¤ºä¾‹",
            "polygon_loading": "åŠ è½½3Då¤šè¾¹å½¢æ•°æ®çš„ç¤ºä¾‹",
            "volume_rendering": "ä½“ç§¯æ¸²æŸ“çš„ç¤ºä¾‹",
            "streamline": "æµçº¿æ¸²æŸ“çš„ç¤ºä¾‹"
        }
        print("  å·²ä½¿ç”¨æ¼”ç¤ºè¯­æ–™åº“")
    else:
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(corpus)} ä¸ªè¯­æ–™åº“é¡¹")

    # ç¬¬2æ­¥ï¼šä¿å­˜è¯­æ–™åº“ä¸ºJSON
    print("\n[æ­¥éª¤2] ä¿å­˜è¯­æ–™åº“...")
    save_corpus_json(corpus, args.corpus_output)

    # ç¬¬3æ­¥ï¼šæ‰§è¡Œæ£€ç´¢
    print("\n[æ­¥éª¤3] æ‰§è¡Œæ£€ç´¢ä»»åŠ¡...")
    all_results = []

    for i, task in enumerate(tasks, 1):
        print(f"\n--- ä»»åŠ¡ {i}/{len(tasks)} ---")
        result = perform_retrieval(task, corpus, args.model)
        all_results.append(result)

        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™æµ
        if i < len(tasks):
            time.sleep(2)

    # ç¬¬4æ­¥ï¼šä¿å­˜ç»“æœ
    print("\n[æ­¥éª¤4] ä¿å­˜ç»“æœ...")
    output_data = {
        "summary": {
            "total_queries": len(all_results),
            "total_time": sum(r.get("elapsed_time", 0) for r in all_results),
            "model_used": args.model,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        },
        "corpus_info": {
            "total_items": len(corpus),
            "items": list(corpus.keys())
        },
        "results": all_results
    }

    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
    except Exception as e:
        print(f"âœ— ä¿å­˜ç»“æœå¤±è´¥: {e}")

    # ç¬¬5æ­¥ï¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("æ£€ç´¢ç»Ÿè®¡")
    print("="*60)
    print(f"æ€»æŸ¥è¯¢æ•°: {len(all_results)}")
    total_time = sum(r.get("elapsed_time", 0) for r in all_results)
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡è€—æ—¶: {total_time/len(all_results):.2f}ç§’" if all_results else "N/A")

    # ç»Ÿè®¡é”™è¯¯
    errors = [r for r in all_results if "error" in r]
    if errors:
        print(f"\nâš  å¤±è´¥æŸ¥è¯¢: {len(errors)}")
        for error_result in errors:
            print(f"  - {error_result['error']}")
    
    # ç»Ÿè®¡æ£€ç´¢åˆ°çš„æ¨¡å—
    total_modules = sum(len(r.get("retrieved_modules", [])) for r in all_results if "retrieved_modules" in r)
    print(f"\nğŸ“Š æ€»æ£€ç´¢æ¨¡å—æ•°: {total_modules}")

    print("\n" + "="*60)
    print("å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
