"""
å¯¹æ¯”æ£€ç´¢ç»“æžœé‡åˆåº¦å’Œæ—¶é—´æˆæœ¬åˆ†æžè„šæœ¬

ä¸»è¦åŠŸèƒ½ï¼š
1. å¯¹æ¯”æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢å’ŒLLMç›´æŽ¥æ£€ç´¢çš„ç»“æžœ
2. åˆ†æžæ—¶é—´æˆæœ¬å·®å¼‚
3. è®¡ç®—æ£€ç´¢ç»“æžœé‡åˆåº¦ï¼ˆç›¸ä¼¼åº¦ï¼‰
4. ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨ï¼ˆæ—¶é—´ã€é‡åˆåº¦ã€æ¨¡å—åˆ†å¸ƒï¼‰
"""

import json
import os
from pickle import GLOBAL
import re
from pathlib import Path
from typing import Dict, List, Tuple, Set
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.patches as mpatches
from collections import defaultdict, Counter

# ä¸­æ–‡å­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

GLOBAL_VARIABLE = 'deepseek_v3_12-19'
class RetrievalComparisonAnalyzer:
    """æ£€ç´¢ç»“æžœå¯¹æ¯”åˆ†æžå™¨
    
    å¯¹æ¯”ã€æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ã€‘vsã€LLMç›´æŽ¥æ£€ç´¢ã€‘çš„ç»“æžœ
    - æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ï¼ˆkeyword-awareï¼‰ï¼šä»Ž case_export_data.json çš„ retrieval_results å­—æ®µæå–
    - LLMç›´æŽ¥æ£€ç´¢ï¼šä»Ž retrieval_results_with_time.json çš„ retrieved_modules å­—æ®µæå–
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æžå™¨"""
        self.keyword_aware_results = {}  # æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ç»“æžœ
        self.llm_results = {}            # LLMç›´æŽ¥æ£€ç´¢ç»“æžœ
        self.comparison_data = {}
        
    def parse_llm_retrieval_results(self, json_path: str = None) -> Dict:
        """
        ä»Žretrieval_results_with_time.jsonæ–‡ä»¶ä¸­è§£æžã€LLMç›´æŽ¥æ£€ç´¢ç»“æžœã€‘
        
        JSONç»“æž„ç¤ºä¾‹ï¼š
        {
          "results": [
            {
              "task": "slice",
              "query": "...",
              "retrieved_modules": ["Filter-ImageSlice", ...],
              "elapsed_time": 15.85,
              "timestamp": "2025-12-19 14:26:22"
            },
            ...
          ]
        }
        """
        results = {}
        
        # å¦‚æžœæ²¡æœ‰æä¾›è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
        if json_path is None:
            json_path = Path(__file__).parent.parent.parent / 'retrieval_results_with_time.json'
        else:
            json_path = Path(json_path)
        
        if not json_path.exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
            return results
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éåŽ†resultsä¸­çš„æ¯ä¸ªæŸ¥è¯¢ç»“æžœ
            query_results = data.get('results', [])
            
            for idx, result in enumerate(query_results):
                # ç›´æŽ¥ä»Žtaskå­—æ®µèŽ·å–ä»»åŠ¡åç§°
                task_name = result.get('task', f'task_{idx}')
                query = result.get('query', '')
                modules = result.get('retrieved_modules', [])
                elapsed_time = result.get('elapsed_time', 0)
                
                results[task_name] = {
                    'time': elapsed_time,
                    'modules': modules,
                    'module_count': len(modules),
                    'query': query[:100]  # ä¿å­˜queryå‰100ä¸ªå­—ç¬¦
                }
            
            print(f"âœ“ æˆåŠŸä»Ž {json_path} åŠ è½½äº† {len(results)} ä¸ªä»»åŠ¡çš„æ£€ç´¢ç»“æžœ")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æžé”™è¯¯: {e}")
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        
        return results
    
    def parse_keyword_aware_retrieval_results(self, export_dir: Path) -> Dict:
        """
        ä»Žgenerated_code_without_rag\gpt_5_with_ragç›®å½•ä¸­è§£æžã€æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ç»“æžœã€‘
        
        éåŽ†å„ä¸ªä»»åŠ¡ç›®å½•ï¼Œè¯»å–case_export_data.jsonä¸­çš„retrieval_resultså­—æ®µ
        è¿™æ˜¯åŸºäºŽå…³é”®è¯å’Œæ¨¡å—ä¿¡æ¯çš„ç»“æž„åŒ–æ£€ç´¢ç»“æžœ
        æ”¯æŒä»Žtitleå­—æ®µå’Œmetadataä¸­çš„retrieval_time_secondsæå–ä¿¡æ¯
        """
        results = {}
        
        # éåŽ†å„ä¸ªä»»åŠ¡ç›®å½•ï¼ˆä¸ŽJSONä¸­çš„taskå­—æ®µä¿æŒä¸€è‡´ï¼‰
        task_dirs = ['slice', 'isosurface', 'streamline', 'volume_rendering', 
                     'cutter', 'stream_tracing', 'streamtracing']
        
        for task_dir_name in task_dirs:
            task_path = export_dir / task_dir_name
            
            if not task_path.exists():
                continue
            
            # å°è¯•è¯»å–case_export_data.json
            json_path = task_path / 'case_export_data.json'
            
            if json_path.exists():
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    # è§£æžretrieval_resultsä¸­çš„æ¨¡å—ä¿¡æ¯
                    retrieval_results = data.get('retrieval_results', [])
                    modules = []
                    titles = []  # æ–°å¢žï¼šæå–titleå­—æ®µ
                    
                    # ç¡®ä¿retrieval_resultsæ˜¯åˆ—è¡¨
                    if not isinstance(retrieval_results, list):
                        print(f"âš ï¸ retrieval_resultsä¸æ˜¯åˆ—è¡¨ç±»åž‹ï¼Œåœ¨{task_path}ä¸­")
                        retrieval_results = []
                    
                    # ä»Žretrieval_resultsæå–æ¨¡å—ä¿¡æ¯
                    for item in retrieval_results:
                        if isinstance(item, dict):
                            # ä¼˜å…ˆä½¿ç”¨ title å­—æ®µï¼ˆæ–°å¢žé€»è¾‘ï¼‰
                            title = item.get('title', '')
                            if title:
                                modules.append(title)
                                titles.append(title)
                            else:
                                # é™çº§æ–¹æ¡ˆï¼šä»Žfile_pathæå–
                                file_path = item.get('file_path', '')
                                if file_path:
                                    import re
                                    match = re.search(r'([A-Z][a-zA-Z]+-[A-Za-z]+)', file_path)
                                    if match:
                                        module_name = match.group(1)
                                        modules.append(module_name)
                    
                    # ä½¿ç”¨ç›´æŽ¥çš„ä»»åŠ¡åç§°ï¼ˆä¸ŽJSONä¸­çš„taskå­—æ®µä¸€è‡´ï¼‰
                    task_name = task_dir_name
                    
                    # åŽ»é‡å’Œæ¸…ç†
                    modules = list(set([m.strip() for m in modules if isinstance(m, str) and m.strip()]))
                    
                    # æ–°å¢žï¼šä»Žmetadataä¸­æå–æ£€ç´¢è€—æ—¶
                    retrieval_time = None
                    metadata = data.get('metadata', {})
                    if isinstance(metadata, dict):
                        retrieval_time = metadata.get('retrieval_time_seconds')
                    
                    results[task_name] = {
                        'time': retrieval_time,  # ä»Žmetadataä¸­æå–çš„å®žé™…è€—æ—¶
                        'modules': modules,  # åŽ»é‡åŽçš„æ¨¡å—åˆ—è¡¨
                        'module_count': len(modules),
                        'retrieval_count': len(retrieval_results),
                        'titles': titles,  # æ–°å¢žï¼šä¿å­˜æ‰€æœ‰title
                        'raw_retrieval_results': retrieval_results  # ä¿å­˜åŽŸå§‹æ•°æ®ä¾›åŽç»­åˆ†æž
                    }
                    
                    # æ‰“å°è°ƒè¯•ä¿¡æ¯
                    time_str = f" (è€—æ—¶: {retrieval_time}s)" if retrieval_time else ""
                    if modules:
                        print(f"  ä»Ž{task_name}ä¸­æ‰¾åˆ°{len(modules)}ä¸ªæ¨¡å—ï¼ˆæ€»è®¡{len(retrieval_results)}æ¡æ£€ç´¢ç»“æžœï¼‰{time_str}")
                    
                except Exception as e:
                    print(f"âŒ è§£æž{json_path}å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
        
        return results
    
    def calculate_overlap(self, keyword_aware_modules: List[str], llm_modules: List[str]) -> Dict:
        """
        è®¡ç®—ä¸¤ä¸ªæ¨¡å—åˆ—è¡¨çš„é‡åˆåº¦
        
        è¿”å›žï¼š
        - overlap: é‡åˆçš„æ¨¡å—
        - overlap_rate: é‡åˆçŽ‡ (é‡åˆæ•° / å¹¶é›†æ•°)
        - similarity: ç›¸ä¼¼åº¦ (é‡åˆæ•° / keyword_awareæ£€ç´¢æ•°)
        """
        keyword_aware_set = set([m.lower().strip() for m in keyword_aware_modules])
        llm_set = set([m.lower().strip() for m in llm_modules])
        
        if not keyword_aware_set or not llm_set:
            return {
                'overlap': [],
                'overlap_count': 0,
                'overlap_rate': 0.0,
                'keyword_aware_only': list(keyword_aware_set),
                'llm_only': list(llm_set),
                'similarity': 0.0
            }
        
        overlap = keyword_aware_set & llm_set
        union = keyword_aware_set | llm_set
        keyword_aware_only = keyword_aware_set - llm_set
        llm_only = llm_set - keyword_aware_set
        
        # é‡åˆçŽ‡ (Jaccardç›¸ä¼¼åº¦)
        overlap_rate = len(overlap) / len(union) if union else 0.0
        # ç›¸ä¼¼åº¦ (keyword_awareæ£€ç´¢ç»“æžœä¸­è¢«LLMè¦†ç›–çš„æ¯”ä¾‹)
        similarity = len(overlap) / len(keyword_aware_set) if keyword_aware_set else 0.0
        
        return {
            'overlap': list(overlap),
            'overlap_count': len(overlap),
            'overlap_rate': overlap_rate,
            'keyword_aware_only': list(keyword_aware_only),
            'llm_only': list(llm_only),
            'similarity': similarity,
            'keyword_aware_count': len(keyword_aware_set),
            'llm_count': len(llm_set),
            'union_count': len(union)
        }
    
    def analyze(self, json_path: str = None, export_dir: Path = None) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„å¯¹æ¯”åˆ†æž
        
        Args:
            json_path: retrieval_results_with_time.json è·¯å¾„ï¼ˆLLMç›´æŽ¥æ£€ç´¢ï¼‰
            export_dir: case_export_data.json æ‰€åœ¨ç›®å½•ï¼ˆæ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ï¼‰
        """
        
        # è§£æžä¸¤ä¸ªæ•°æ®æº
        self.llm_results = self.parse_llm_retrieval_results(json_path)
        if export_dir:
            self.keyword_aware_results = self.parse_keyword_aware_retrieval_results(export_dir)
        
        # å¯¹æ¯”åˆ†æž
        self.comparison_data = {}
        
        # è§„èŒƒåŒ–ä»»åŠ¡åç§°ç”¨äºŽåŒ¹é…
        def normalize_name(name):
            return name.lower().replace('_', '').replace('-', '').replace(' ', '')
        
        keyword_aware_norm = {normalize_name(k): k for k in self.keyword_aware_results.keys()}
        llm_norm = {normalize_name(k): k for k in self.llm_results.keys()}
        
        # åˆå¹¶æ‰€æœ‰ä»»åŠ¡
        all_tasks = set(keyword_aware_norm.keys()) | set(llm_norm.keys())
        
        for task_norm in all_tasks:
            task_key = task_norm
            
            keyword_aware_key = keyword_aware_norm.get(task_norm)
            llm_key = llm_norm.get(task_norm)
            
            keyword_aware_data = self.keyword_aware_results.get(keyword_aware_key, {}) if keyword_aware_key else {}
            llm_data = self.llm_results.get(llm_key, {}) if llm_key else {}
            
            # è®¡ç®—é‡åˆåº¦
            keyword_aware_modules = keyword_aware_data.get('modules', [])
            llm_modules = llm_data.get('modules', [])
            
            overlap_analysis = self.calculate_overlap(keyword_aware_modules, llm_modules)
            
            self.comparison_data[task_key] = {
                'keyword_aware': keyword_aware_data,
                'llm': llm_data,
                'overlap': overlap_analysis,
                'display_name': keyword_aware_key or llm_key or task_key
            }
        
        return self.comparison_data
    
    def print_summary(self):
        """æ‰“å°å¯¹æ¯”æ‘˜è¦"""
        print("\n" + "="*80)
        print("ã€æ£€ç´¢ç»“æžœå¯¹æ¯”åˆ†æžæ‘˜è¦ã€‘")
        print("å¯¹æ¯”ï¼šæ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢(keyword-aware) vs LLMç›´æŽ¥æ£€ç´¢")
        print("="*80)
        
        for task_key, data in self.comparison_data.items():
            display_name = data['display_name']
            keyword_aware = data['keyword_aware']
            llm = data['llm']
            overlap = data['overlap']
            
            print(f"\nðŸ“Œ {display_name.upper()}")
            print("-" * 60)
            
            # æ—¶é—´å¯¹æ¯”ï¼ˆåŒ…æ‹¬æ–°æå–çš„è€—æ—¶æ•°æ®ï¼‰
            keyword_aware_time = keyword_aware.get('time')
            llm_time = llm.get('time')
            
            print(f"\n  â±ï¸  æ—¶é—´æˆæœ¬å¯¹æ¯”:")
            if keyword_aware_time:
                print(f"     â€¢ å…³é”®è¯æ£€ç´¢ï¼ˆkeyword-awareï¼‰: {keyword_aware_time:.2f}s")
            else:
                print(f"     â€¢ å…³é”®è¯æ£€ç´¢ï¼ˆkeyword-awareï¼‰: N/A")
            if llm_time:
                print(f"     â€¢ LLMç›´æŽ¥æ£€ç´¢: {llm_time:.2f}s")
            else:
                print(f"     â€¢ LLMç›´æŽ¥æ£€ç´¢: N/A")
            
            # æ—¶é—´å¯¹æ¯”åˆ†æž
            if keyword_aware_time and llm_time:
                diff = abs(llm_time - keyword_aware_time)
                percent = (diff / min(keyword_aware_time, llm_time)) * 100 if min(keyword_aware_time, llm_time) > 0 else 0
                if llm_time > keyword_aware_time:
                    print(f"     â€¢ LLMæ–¹æ³•æ…¢ {diff:.2f}s ({percent:.1f}%)")
                else:
                    print(f"     â€¢ å…³é”®è¯æ–¹æ³•æ…¢ {diff:.2f}s ({percent:.1f}%)")
            
            # æ¨¡å—å¯¹æ¯”
            keyword_aware_count = keyword_aware.get('module_count', 0)
            llm_count = llm.get('module_count', 0)
            overlap_count = overlap.get('overlap_count', 0)
            similarity = overlap.get('similarity', 0.0)
            overlap_rate = overlap.get('overlap_rate', 0.0)
            
            print(f"\n  ðŸ§© æ¨¡å—ç»Ÿè®¡:")
            print(f"     â€¢ å…³é”®è¯æ£€ç´¢: {keyword_aware_count} ä¸ªæ¨¡å—")
            print(f"     â€¢ LLMç›´æŽ¥æ£€ç´¢: {llm_count} ä¸ªæ¨¡å—")
            print(f"     â€¢ é‡åˆæ¨¡å—: {overlap_count} ä¸ª")
            print(f"     â€¢ è¦†ç›–çŽ‡: {similarity*100:.1f}% (LLMè¦†ç›–å…³é”®è¯ç»“æžœçš„æ¯”ä¾‹)")
            print(f"     â€¢ é‡åˆçŽ‡: {overlap_rate*100:.1f}% (Jaccardç›¸ä¼¼åº¦)")
            
            # ç‹¬æœ‰æ¨¡å—
            keyword_aware_only = overlap.get('keyword_aware_only', [])
            llm_only = overlap.get('llm_only', [])
            
            if keyword_aware_only:
                print(f"\n  ðŸ”´ å…³é”®è¯æ£€ç´¢ç‹¬æœ‰æ¨¡å— ({len(keyword_aware_only)}):")
                for m in keyword_aware_only[:5]:
                    print(f"     - {m}")
                if len(keyword_aware_only) > 5:
                    print(f"     ... è¿˜æœ‰ {len(keyword_aware_only)-5} ä¸ª")
            
            if llm_only:
                print(f"\n  ðŸŸ¢ LLMæ£€ç´¢ç‹¬æœ‰æ¨¡å— ({len(llm_only)}):")
                for m in llm_only[:5]:
                    print(f"     - {m}")
                if len(llm_only) > 5:
                    print(f"     ... è¿˜æœ‰ {len(llm_only)-5} ä¸ª")
    
    def plot_time_comparison(self, output_file='retrieval_time_comparison.png'):
        """ç»˜åˆ¶æ—¶é—´æˆæœ¬å¯¹æ¯”å›¾ - åŒ…æ‹¬ä¸¤ç§æ£€ç´¢æ–¹æ³•çš„è€—æ—¶å¯¹æ¯”å’Œæ•ˆçŽ‡åˆ†æž"""
        
        # æ³¨å…¥å…¨å±€å˜é‡åˆ°æ–‡ä»¶å
        if not output_file.startswith(GLOBAL_VARIABLE):
            base_name = Path(output_file).name
            output_file = str(Path(output_file).parent / f"{GLOBAL_VARIABLE}_{base_name}")
        
        # æå–æœ‰æ—¶é—´æ•°æ®çš„ä»»åŠ¡
        tasks = []
        keyword_aware_times = []
        llm_times = []
        speedup_ratios = []
        
        for task_key, data in self.comparison_data.items():
            display_name = data['display_name']
            keyword_aware_time = data['keyword_aware'].get('time')
            llm_time = data['llm'].get('time')
            
            if keyword_aware_time or llm_time:
                tasks.append(display_name)
                kw_time = keyword_aware_time if keyword_aware_time else 0
                llm_t = llm_time if llm_time else 0
                
                keyword_aware_times.append(kw_time)
                llm_times.append(llm_t)
                
                # è®¡ç®—åŠ é€Ÿæ¯”
                if kw_time > 0 and llm_t > 0:
                    speedup = llm_t / kw_time
                    speedup_ratios.append(speedup)
                else:
                    speedup_ratios.append(0)
        
        if not tasks:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æ•°æ®ç”¨äºŽç»˜å›¾")
            return
        
        # åˆ›å»ºåŒYè½´å›¾è¡¨ï¼šå·¦è¾¹æ˜¯æ—¶é—´ï¼Œå³è¾¹æ˜¯åŠ é€Ÿæ¯”
        fig, ax1 = plt.subplots(figsize=(12, 6), dpi=100)
        
        x = np.arange(len(tasks))
        width = 0.35
        
        # å·¦Yè½´ï¼šæ—¶é—´æˆæœ¬
        bars1 = ax1.bar(x - width/2, keyword_aware_times, width, label='å…³é”®è¯æ£€ç´¢(keyword-aware)', 
                        color='#4E79A7', alpha=0.8, edgecolor='black', linewidth=1.5)
        bars2 = ax1.bar(x + width/2, llm_times, width, label='LLMç›´æŽ¥æ£€ç´¢', 
                        color='#F28E2B', alpha=0.8, edgecolor='black', linewidth=1.5)
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                if height > 0:
                    ax1.text(bar.get_x() + bar.get_width()/2., height,
                            f'{height:.2f}s',
                            ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        ax1.set_xlabel('ä»»åŠ¡åç§°', fontsize=12, fontweight='bold')
        ax1.set_ylabel('æ—¶é—´æˆæœ¬ (ç§’)', fontsize=12, fontweight='bold', color='black')
        ax1.tick_params(axis='y', labelcolor='black')
        ax1.set_xticks(x)
        ax1.set_xticklabels(tasks, rotation=45, ha='right')
        ax1.grid(axis='y', alpha=0.3, linestyle='--')
        
        # å³Yè½´ï¼šåŠ é€Ÿæ¯”
        ax2 = ax1.twinx()
        if any(speedup_ratios):
            line = ax2.plot(x, speedup_ratios, 'ro-', linewidth=2.5, markersize=10,
                           label='LLMç›¸å¯¹åŠ é€Ÿæ¯”', markerfacecolor='white', markeredgewidth=2)
            
            # åœ¨çº¿ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (xi, yi) in enumerate(zip(x, speedup_ratios)):
                if yi > 0:
                    ax2.text(xi, yi + 0.05, f'{yi:.2f}x', ha='center', va='bottom', 
                            fontsize=9, fontweight='bold', color='red')
            
            ax2.set_ylabel('åŠ é€Ÿæ¯” (LLMæ—¶é—´/å…³é”®è¯æ—¶é—´)', fontsize=12, fontweight='bold', color='red')
            ax2.tick_params(axis='y', labelcolor='red')
            ax2.axhline(y=1, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='ç›¸ç­‰çº¿')
        
        ax1.set_title('æ£€ç´¢æ—¶é—´æˆæœ¬å¯¹æ¯”åˆ†æž (å…³é”®è¯æ£€ç´¢ vs LLMæ£€ç´¢)', fontsize=14, fontweight='bold', pad=20)
        
        # åˆå¹¶å›¾ä¾‹
        lines1, labels1 = ax1.get_legend_handles_labels()
        if any(speedup_ratios):
            lines2, labels2 = ax2.get_legend_handles_labels()
            ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=11, loc='upper left', framealpha=0.95)
        else:
            ax1.legend(fontsize=11, loc='upper left')
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=100, bbox_inches='tight')
        print(f"âœ“ æ—¶é—´å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_file}")
        plt.close()
    
    def plot_overlap_comparison(self, output_file='retrieval_overlap_comparison.png'):
        """ç»˜åˆ¶æ£€ç´¢ç»“æžœè¦†ç›–çŽ‡å’Œæ¨¡å—å æ¯”å¯¹æ¯”å›¾"""
            
        # æ³¨å…¥å…¨å±€å˜é‡åˆ°æ–‡ä»¶å
        if not output_file.startswith(GLOBAL_VARIABLE):
            base_name = Path(output_file).name
            output_file = str(Path(output_file).parent / f"{GLOBAL_VARIABLE}_{base_name}")
            
        tasks = []
        similarities = []
        keyword_aware_ratios = []  # æ–°å¢žï¼šæ¨¡å—æ„ŸçŸ¥å LLMçš„æ¯”ä¾‹
        keyword_aware_counts = []
        llm_counts = []
            
        for task_key, data in self.comparison_data.items():
            display_name = data['display_name']
            overlap = data['overlap']
                
            tasks.append(display_name)
            similarities.append(overlap.get('similarity', 0.0) * 100)
            
            # æ–°å¢žï¼šè®¡ç®—æ¨¡å—æ„ŸçŸ¥å LLMçš„æ¯”ä¾‹
            keyword_aware_count = overlap.get('keyword_aware_count', 0)
            llm_count = overlap.get('llm_count', 0)
            if llm_count > 0:
                ratio = (keyword_aware_count / llm_count) * 100
            else:
                ratio = 0
            keyword_aware_ratios.append(ratio)
            
            keyword_aware_counts.append(keyword_aware_count)
            llm_counts.append(llm_count)
            
        if not tasks:
            print("âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºŽç»˜åˆ¶è¦†ç›–çŽ‡å¯¹æ¯”å›¾")
            return
            
        # åˆ›å»ºåŒ Y è½´å›¾è¡¨
        try:
            fig, ax1 = plt.subplots(figsize=(12, 6), dpi=100)
                
            x = np.arange(len(tasks))
            width = 0.35
                
            # å·¦ Y è½´ï¼šè¦†ç›–çŽ‡å’Œæ¨¡å—æ„ŸçŸ¥å LLMçš„æ¯”ä¾‹
            bars1 = ax1.bar(x - width/2, similarities, width, label='è¦†ç›–çŽ‡ (LLMè¦†ç›–å…³é”®è¯)', 
                            color='#59A14F', alpha=0.8, edgecolor='black', linewidth=1.5)
            bars2 = ax1.bar(x + width/2, keyword_aware_ratios, width, label='æ¨¡å—æ„ŸçŸ¥å LLMçš„æ¯”ä¾‹', 
                            color='#E15759', alpha=0.8, edgecolor='black', linewidth=1.5)
                
            ax1.set_xlabel('ä»»åŠ¡åç§°', fontsize=12, fontweight='bold')
            ax1.set_ylabel('æ¯”ä¾‹ (%)', fontsize=12, fontweight='bold', color='black')
            ax1.set_xticks(x)
            ax1.set_xticklabels(tasks, rotation=45, ha='right')
            ax1.tick_params(axis='y', labelcolor='black')
            ax1.set_ylim(0, 105)
            ax1.grid(axis='y', alpha=0.3, linestyle='--')
                
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    if height > 0:
                        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,
                                f'{height:.0f}%',
                                ha='center', va='bottom', fontsize=9, fontweight='bold')
                
            # å³ Y è½´ï¼šæ¨¡å—è®¡æ•°
            ax2 = ax1.twinx()
            line1 = ax2.plot(x, keyword_aware_counts, 'o-', linewidth=2.5, markersize=8,
                            label='æ¨¡å—æ„ŸçŸ¥æ¨¡å—æ•°', color='#4E79A7', markerfacecolor='white', markeredgewidth=2)
            line2 = ax2.plot(x, llm_counts, 's-', linewidth=2.5, markersize=8,
                            label='LLMæ¨¡å—æ•°', color='#F28E2B', markerfacecolor='white', markeredgewidth=2)
                
            ax2.set_ylabel('æ¨¡å—æ•°é‡', fontsize=12, fontweight='bold', color='black')
            ax2.tick_params(axis='y', labelcolor='black')
                
            ax1.set_title('æ£€ç´¢ç»“æžœè¦†ç›–çŽ‡å’Œæ¨¡å—å æ¯”å¯¹æ¯”åˆ†æž', fontsize=14, fontweight='bold', pad=20)
                
            # åˆå¹¶å›¾ä¾‹ï¼ˆä»…å½“æœ‰æœ‰æ•ˆå¥æŸ„æ—¶ï¼‰
            bars_labels = [b.get_label() for b in [bars1, bars2]]
            bars_handles = [bars1, bars2]
                
            lines_labels = ['æ¨¡å—æ„ŸçŸ¥æ¨¡å—æ•°', 'LLMæ¨¡å—æ•°']
            lines_handles = [l for l in line1 + line2 if l is not None]  # è¿‡æ»¤æœ‰æ•ˆçš„å¥æŸ„
                
            if lines_handles:
                ax1.legend(bars_handles + lines_handles, bars_labels + lines_labels,
                          loc='upper left', fontsize=10, framealpha=0.95)
            else:
                ax1.legend(bars_handles, bars_labels, loc='upper left', fontsize=10, framealpha=0.95)
                
            plt.tight_layout()
            plt.savefig(output_file, dpi=100, bbox_inches='tight')
            print(f"âœ“ è¦†ç›–çŽ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_file}")
            plt.close()
        except Exception as e:
            print(f"âš ï¸ ç»˜åˆ¶è¦†ç›–çŽ‡å¯¹æ¯”å›¾å¤±è´¥: {e}")
            plt.close()
    
    def plot_module_distribution(self, output_file='module_distribution_comparison.png'):
        """ç»˜åˆ¶æ¨¡å—åˆ†å¸ƒå¯¹æ¯”å›¾"""
            
        # æ³¨å…¥å…¨å±€å˜é‡åˆ°æ–‡ä»¶å
        if not output_file.startswith(GLOBAL_VARIABLE):
            base_name = Path(output_file).name
            output_file = str(Path(output_file).parent / f"{GLOBAL_VARIABLE}_{base_name}")
            
        # æ”¶é›†æ‰€æœ‰æ¨¡å—åŠå…¶åœ¨å„ä»»åŠ¡ä¸­çš„å‡ºçŽ°é¢‘çŽ‡
        keyword_aware_module_freq = Counter()
        llm_module_freq = Counter()
            
        for task_key, data in self.comparison_data.items():
            for module in data['keyword_aware'].get('modules', []):
                keyword_aware_module_freq[module] += 1
            for module in data['llm'].get('modules', []):
                llm_module_freq[module] += 1
            
        # èŽ·å–Top 10æ¨¡å—
        top_keyword_aware = keyword_aware_module_freq.most_common(10)
        top_llm = llm_module_freq.most_common(10)
            
        # åˆ›å»ºå¹¶è¡Œæ°±çŠ¶å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=100)
            
        # æ¨¡å—æ„ŸçŸ¥æ£€ç´¢æ¨¡å—åˆ†å¸ƒ
        if top_keyword_aware:
            modules1, counts1 = zip(*top_keyword_aware)
            y_pos = np.arange(len(modules1))
            ax1.barh(y_pos, counts1, color='#4E79A7', alpha=0.8, edgecolor='black', linewidth=1.5)
            ax1.set_yticks(y_pos)
            ax1.set_yticklabels([m[:30] for m in modules1], fontsize=10)
            ax1.set_xlabel('å‡ºçŽ°é¢‘çŽ‡', fontsize=11, fontweight='bold')
            ax1.set_title('æ¨¡å—æ„ŸçŸ¥æ£€ç´¢ - é«˜é¢‘æ¨¡å— TOP 10', fontsize=12, fontweight='bold')
            ax1.grid(axis='x', alpha=0.3, linestyle='--')
                
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (m, c) in enumerate(top_keyword_aware):
                ax1.text(c + 0.1, i, f'{c}', va='center', fontsize=9, fontweight='bold')
            
        # LLMç›´æŽ¥æ£€ç´¢æ¨¡å—åˆ†å¸ƒ
        if top_llm:
            modules2, counts2 = zip(*top_llm)
            y_pos = np.arange(len(modules2))
            ax2.barh(y_pos, counts2, color='#F28E2B', alpha=0.8, edgecolor='black', linewidth=1.5)
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels([m[:30] for m in modules2], fontsize=10)
            ax2.set_xlabel('å‡ºçŽ°é¢‘çŽ‡', fontsize=11, fontweight='bold')
            ax2.set_title('LLMç›´æŽ¥æ£€ç´¢ - é«˜é¢‘æ¨¡å— TOP 10', fontsize=12, fontweight='bold')
            ax2.grid(axis='x', alpha=0.3, linestyle='--')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (m, c) in enumerate(top_llm):
                ax2.text(c + 0.1, i, f'{c}', va='center', fontsize=9, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=100, bbox_inches='tight')
        print(f"âœ“ æ¨¡å—åˆ†å¸ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {output_file}")
        plt.close()
    
    def export_to_json(self, output_dir=None, output_file='retrieval_comparison_result.json'):
        """å¯¼å‡ºåˆ†æžç»“æžœä¸ºJSON - åŒ…å«å®Œæ•´çš„æ—¶é—´å’Œæ ‡é¢˜ä¿¡æ¯"""
        
        # æ³¨å…¥å…¨å±€å˜é‡åˆ°æ–‡ä»¶å
        if not output_file.startswith(GLOBAL_VARIABLE):
            base_name = Path(output_file).name
            output_file = f"{GLOBAL_VARIABLE}_{base_name}"
        
        export_data = {
            'summary': {
                'analysis_timestamp': str(__import__('datetime').datetime.now()),
                'comparison_type': 'keyword-aware(å…³é”®è¯æ£€ç´¢) vs LLM(LLMç›´æŽ¥æ£€ç´¢)',
                'total_tasks': len(self.comparison_data)
            },
            'detailed_results': {}
        }
        
        # ç»Ÿè®¡æ•°æ®
        total_keyword_aware_time = 0
        total_llm_time = 0
        total_tasks_with_time = 0
        
        for task_key, data in self.comparison_data.items():
            overlap = data['overlap']
            keyword_aware = data['keyword_aware']
            llm = data['llm']
            keyword_aware_modules = keyword_aware.get('modules', [])
            llm_modules = llm.get('modules', [])
            
            kw_time = keyword_aware.get('time')
            llm_time = llm.get('time')
            
            # ç´¯è®¡æœ‰æ•ˆæ—¶é—´æ•°æ®
            if kw_time:
                total_keyword_aware_time += kw_time
            if llm_time:
                total_llm_time += llm_time
            if kw_time or llm_time:
                total_tasks_with_time += 1
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup_ratio = None
            if kw_time and llm_time and kw_time > 0:
                speedup_ratio = round(llm_time / kw_time, 2)
            
            export_data['detailed_results'][data['display_name']] = {
                'time_analysis': {
                    'keyword_aware_time_seconds': kw_time,
                    'llm_time_seconds': llm_time,
                    'speedup_ratio': speedup_ratio,  # æ–°å¢žï¼šåŠ é€Ÿæ¯”
                    'time_faster_method': 'keyword-aware' if (kw_time and llm_time and kw_time < llm_time) else ('llm' if (kw_time and llm_time and llm_time < kw_time) else None)
                },
                'module_analysis': {
                    'keyword_aware_modules_count': overlap.get('keyword_aware_count', 0),
                    'keyword_aware_modules': [m.lower() for m in keyword_aware_modules],
                    'keyword_aware_titles': keyword_aware.get('titles', []),  # æ–°å¢žï¼štitleå­—æ®µ
                    'llm_modules_count': overlap.get('llm_count', 0),
                    'llm_modules': [m.lower() for m in llm_modules],
                    'overlap_count': overlap.get('overlap_count', 0),
                    'overlap_modules': overlap.get('overlap', []),
                },
                'similarity_metrics': {
                    'coverage_rate': round(overlap.get('similarity', 0.0) * 100, 2),  # LLMè¦†ç›–å…³é”®è¯ç»“æžœçš„æ¯”ä¾‹
                    'keyword_aware_to_llm_ratio': round((overlap.get('keyword_aware_count', 0) / overlap.get('llm_count', 1)) * 100, 2),  # æ¨¡å—æ„ŸçŸ¥æ¨¡å—æ•°å LLMæ¨¡å—æ•°çš„æ¯”ä¾‹
                },
                'unique_modules': {
                    'keyword_aware_only': overlap.get('keyword_aware_only', [])[:10],
                    'llm_only': overlap.get('llm_only', [])[:10],
                }
            }
        
        # æ·»åŠ æ€»ä½“ç»Ÿè®¡
        export_data['summary']['total_keyword_aware_time_seconds'] = round(total_keyword_aware_time, 2)
        export_data['summary']['total_llm_time_seconds'] = round(total_llm_time, 2)
        export_data['summary']['average_speedup_ratio'] = round(total_llm_time / total_keyword_aware_time, 2) if total_keyword_aware_time > 0 else None
        
        if output_dir is None:
            output_dir = Path.cwd()
        else:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ åˆ†æžç»“æžœå·²å¯¼å‡ºåˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°
    
    å¯¹æ¯”ä¸¤ç§æ£€ç´¢æ–¹å¼ï¼š
    1. æ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ï¼ˆkeyword-aware retrievalï¼‰ï¼šä»Ž generated_code_without_rag/gpt_5_with_rag/{task}/case_export_data.json çš„ retrieval_results å­—æ®µ
       - åŸºäºŽå…³é”®è¯å’Œæ¨¡å—ä¿¡æ¯çš„ç»“æž„åŒ–æ£€ç´¢
       - æ”¯æŒä»Žtitleå’Œmetadataä¸­æå–ä¿¡æ¯
    2. LLMç›´æŽ¥æ£€ç´¢ï¼ˆLLM direct retrievalï¼‰ï¼šä»Ž retrieval_results_with_time.json çš„ retrieved_modules å­—æ®µ
       - ç”±å¤§æ¨¡åž‹è¿›è¡Œè¯­ä¹‰ç†è§£å’Œæ£€ç´¢
    
    è¾“å‡ºå†…å®¹ï¼š
    - æ—¶é—´æˆæœ¬å¯¹æ¯”ï¼ˆå«åŠ é€Ÿæ¯”åˆ†æžï¼‰
    - æ£€ç´¢ç»“æžœé‡åˆåº¦åˆ†æž
    - æ¨¡å—åˆ†å¸ƒå¯¹æ¯”
    - è¯¦ç»†çš„JSONæŠ¥å‘Š
    """
    
    # å·¥ä½œç›®å½•
    work_dir = Path(__file__).parent.parent.parent
    os.chdir(work_dir)
    
    print("ðŸš€ å¼€å§‹æ£€ç´¢ç»“æžœå¯¹æ¯”åˆ†æž...")
    print("å¯¹æ¯”ï¼šæ¨¡å—æ„ŸçŸ¥çš„å…³é”®è¯æ£€ç´¢ vs LLMç›´æŽ¥æ£€ç´¢\n")
    
    # è¯»å–LLMç›´æŽ¥æ£€ç´¢ç»“æžœJSON
    json_file = work_dir / 'retrieval_results_with_time.json'
    if not json_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {json_file}")
        return
    
    # è¯»å–æ¨¡å—æ„ŸçŸ¥æ£€ç´¢ç»“æžœç›®å½•
    export_dir = work_dir / "experiment_results" / "generated_code_without_rag" / "gpt_5_with_rag"
    if not export_dir.exists():
        print(f"âš ï¸ æ¨¡å—æ„ŸçŸ¥æ£€ç´¢ç»“æžœç›®å½•ä¸å­˜åœ¨: {export_dir}")
        export_dir = None
    
    # æ‰§è¡Œåˆ†æž
    analyzer = RetrievalComparisonAnalyzer()
    
    # è§£æžLLMç›´æŽ¥æ£€ç´¢ç»“æžœ
    analyzer.llm_results = analyzer.parse_llm_retrieval_results(str(json_file))
    print(f"\nâœ… æˆåŠŸåŠ è½½LLMç›´æŽ¥æ£€ç´¢ç»“æžœï¼š{len(analyzer.llm_results)} ä¸ªä»»åŠ¡")
    
    # è§£æžæ¨¡å—æ„ŸçŸ¥æ£€ç´¢ç»“æžœ
    if export_dir:
        print(f"\næ­£åœ¨ä»Žæ¨¡å—æ„ŸçŸ¥æ£€ç´¢ç»“æžœä¸­æå–æ¨¡å—ä¿¡æ¯ï¼ˆåŒ…æ‹¬titleå’Œè€—æ—¶ï¼‰...")
        analyzer.keyword_aware_results = analyzer.parse_keyword_aware_retrieval_results(export_dir)
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å—æ„ŸçŸ¥æ£€ç´¢ç»“æžœï¼š{len(analyzer.keyword_aware_results)} ä¸ªä»»åŠ¡")
        # ç»Ÿè®¡è€—æ—¶ä¿¡æ¯
        time_available = sum(1 for r in analyzer.keyword_aware_results.values() if r.get('time'))
        print(f"   å…¶ä¸­ {time_available} ä¸ªä»»åŠ¡åŒ…å«è€—æ—¶æ•°æ®\n")
    else:
        analyzer.keyword_aware_results = {}
    
    # æ‰§è¡Œå¯¹æ¯”åˆ†æžï¼ˆåˆå¹¶ç»“æžœï¼‰
    analyzer.comparison_data = {}
    
    # è§„èŒƒåŒ–ä»»åŠ¡åç§°ç”¨äºŽåŒ¹é…
    def normalize_name(name):
        return name.lower().replace('_', '').replace('-', '').replace(' ', '')
    
    keyword_aware_norm = {normalize_name(k): k for k in analyzer.keyword_aware_results.keys()}
    llm_norm = {normalize_name(k): k for k in analyzer.llm_results.keys()}
    
    # åˆå¹¶æ‰€æœ‰ä»»åŠ¡
    all_tasks = set(keyword_aware_norm.keys()) | set(llm_norm.keys())
    
    for task_norm in all_tasks:
        task_key = task_norm
        
        keyword_aware_key = keyword_aware_norm.get(task_norm)
        llm_key = llm_norm.get(task_norm)
        
        keyword_aware_data = analyzer.keyword_aware_results.get(keyword_aware_key, {}) if keyword_aware_key else {}
        llm_data = analyzer.llm_results.get(llm_key, {}) if llm_key else {}
        
        # è®¡ç®—é‡åˆåº¦
        keyword_aware_modules = keyword_aware_data.get('modules', [])
        llm_modules = llm_data.get('modules', [])
        
        overlap_analysis = analyzer.calculate_overlap(keyword_aware_modules, llm_modules)
        
        analyzer.comparison_data[task_key] = {
            'keyword_aware': keyword_aware_data,
            'llm': llm_data,
            'overlap': overlap_analysis,
            'display_name': keyword_aware_key or llm_key or task_key
        }
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()
    
    # ç”Ÿæˆå›¾è¡¨
    print("\nðŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
    
    # èŽ·å–analysç›®å½•è·¯å¾„
    analys_dir = Path(__file__).parent
    
    analyzer.plot_time_comparison(str(analys_dir / 'retrieval_time_comparison_with_speedup.png'))
    analyzer.plot_overlap_comparison(str(analys_dir / 'retrieval_overlap_comparison.png'))
    analyzer.plot_module_distribution(str(analys_dir / 'module_distribution_comparison.png'))
    analyzer.export_to_json(analys_dir, 'retrieval_comparison_result_detailed.json')
    
    print("\nâœ… åˆ†æžå®Œæˆï¼")
    print("\nðŸ“Š ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶ï¼š")
    print(f"   â€¢ æ—¶é—´æˆæœ¬å¯¹æ¯”å›¾ï¼ˆå«åŠ é€Ÿæ¯”ï¼‰: retrieval_time_comparison_with_speedup.png")
    print(f"   â€¢ é‡åˆåº¦å¯¹æ¯”å›¾: retrieval_overlap_comparison.png")
    print(f"   â€¢ æ¨¡å—åˆ†å¸ƒå¯¹æ¯”å›¾: module_distribution_comparison.png")
    print(f"   â€¢ è¯¦ç»†åˆ†æžç»“æžœï¼ˆJSONï¼‰: retrieval_comparison_result_detailed.json")
    print(f"\nðŸ’¡ è¯´æ˜Žï¼š")
    print(f"   â€¢ å…³é”®è¯æ£€ç´¢ï¼ˆkeyword-awareï¼‰ï¼šåŸºäºŽVTK.jsæ¨¡å—çš„ç»“æž„åŒ–æ£€ç´¢")
    print(f"   â€¢ LLMç›´æŽ¥æ£€ç´¢ï¼šç”±å¤§æ¨¡åž‹è¿›è¡Œè¯­ä¹‰ç†è§£å’Œæ£€ç´¢")
    print(f"   â€¢ åŠ é€Ÿæ¯” > 1.0ï¼šLLMæ–¹æ³•æ›´å¿«")
    print(f"   â€¢ åŠ é€Ÿæ¯” < 1.0ï¼šå…³é”®è¯æ–¹æ³•æ›´å¿«")


if __name__ == '__main__':
    main()
