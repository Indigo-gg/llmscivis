"""
RAG ä¸¤é˜¶æ®µæ£€ç´¢ç»“æœç»Ÿè®¡åˆ†æè„šæœ¬

ä¸»è¦åŠŸèƒ½ï¼š
1. åˆ†æç¬¬ä¸€é˜¶æ®µåˆç­›ç»“æœå’Œç¬¬äºŒé˜¶æ®µé‡æ’åºç»“æœ
2. ç»Ÿè®¡åœ¨åˆç­›ä¸­å‘ç°ä½†åœ¨é‡æ’åºä¸­ä¸¢å¤±çš„ç»“æœ
3. åˆ†æä¸¢å¤±ç»“æœçš„æ¨¡å—ä¿¡æ¯å’Œç›¸ä¼¼åº¦
4. ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
"""

import json
import pandas as pd
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Set
import matplotlib.pyplot as plt
import numpy as np

# ä¸­æ–‡å­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class RetrieverAnalyzer:
    """æ£€ç´¢ç»“æœåˆ†æå™¨"""
    
    def __init__(self, json_file: str = None, excel_file: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            json_file: åŒ…å«æ£€ç´¢ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„
            excel_file: åŒ…å«ä¸¤é˜¶æ®µç»“æœçš„Excelæ–‡ä»¶è·¯å¾„
        """
        self.json_file = json_file
        self.excel_file = excel_file
        self.json_data = None
        self.stage1_df = None  # åˆç­›ç»“æœ
        self.stage2_df = None  # é‡æ’åºç»“æœ
        self.analysis_results = {}
        
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½JSONå’ŒExcelæ•°æ®"""
        # åŠ è½½JSONæ–‡ä»¶
        if self.json_file and Path(self.json_file).exists():
            try:
                with open(self.json_file, 'r', encoding='utf-8') as f:
                    self.json_data = json.load(f)
                print(f"âœ“ å·²åŠ è½½JSONæ–‡ä»¶: {self.json_file}")
                print(f"  - åŒ…å« {len(self.json_data)} ä¸ªæŸ¥è¯¢ç»“æœ")
            except Exception as e:
                print(f"âœ— åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        
        # åŠ è½½Excelæ–‡ä»¶
        if self.excel_file and Path(self.excel_file).exists():
            try:
                xls = pd.ExcelFile(self.excel_file)
                print(f"âœ“ å·²åŠ è½½Excelæ–‡ä»¶: {self.excel_file}")
                print(f"  - Sheetåˆ—è¡¨: {xls.sheet_names}")
                
                # è¯»å–ä¸¤ä¸ªsheet
                for sheet_name in xls.sheet_names:
                    if 'Initial' in sheet_name or 'Stage 1' in sheet_name:
                        self.stage1_df = pd.read_excel(self.excel_file, sheet_name=sheet_name)
                        print(f"  - {sheet_name}: {len(self.stage1_df)} è¡Œ")
                    elif 'Reranked' in sheet_name or 'Stage 2' in sheet_name:
                        self.stage2_df = pd.read_excel(self.excel_file, sheet_name=sheet_name)
                        print(f"  - {sheet_name}: {len(self.stage2_df)} è¡Œ")
            except Exception as e:
                print(f"âœ— åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {e}")
    
    def analyze_loss_between_stages(self) -> Dict:
        """
        åˆ†æåˆç­›é˜¶æ®µä¸­å‘ç°ä½†é‡æ’åºé˜¶æ®µä¸¢å¤±çš„ç»“æœ
        
        Returns:
            Dict: åŒ…å«ä¸¢å¤±ç»Ÿè®¡çš„å­—å…¸
        """
        if self.stage1_df is None or self.stage2_df is None:
            print("âœ— ç¼ºå°‘Excelæ•°æ®")
            return {}
        
        result = {
            'total_stage1': 0,
            'total_stage2': 0,
            'lost_items': [],
            'lost_count_by_query': defaultdict(int),
            'module_analysis': defaultdict(lambda: {'count': 0, 'avg_similarity': 0, 'examples': []}),
            'similarity_distribution': []
        }
        
        # æŒ‰æŸ¥è¯¢åˆ†ç»„ç»Ÿè®¡
        stage1_by_query = self.stage1_df.groupby('Query Index')
        stage2_by_query = self.stage2_df.groupby('Query Index')
        
        for query_idx in stage1_by_query.groups.keys():
            stage1_group = stage1_by_query.get_group(query_idx)
            stage2_group = stage2_by_query.get_group(query_idx) if query_idx in stage2_by_query.groups else pd.DataFrame()
            
            result['total_stage1'] += len(stage1_group)
            result['total_stage2'] += len(stage2_group)
            
            # è·å–æ–‡ä»¶è·¯å¾„é›†åˆ
            stage1_files = set(stage1_group['File Path'].dropna().unique())
            stage2_files = set(stage2_group['File Path'].dropna().unique()) if not stage2_group.empty else set()
            
            # æ‰¾å‡ºä¸¢å¤±çš„æ–‡ä»¶
            lost_files = stage1_files - stage2_files
            result['lost_count_by_query'][query_idx] = len(lost_files)
            
            # è¯¦ç»†åˆ†æä¸¢å¤±çš„ç»“æœ
            for _, row in stage1_group.iterrows():
                if row['File Path'] in lost_files:
                    lost_item = {
                        'query_index': query_idx,
                        'query_description': row.get('Query Description', 'N/A'),
                        'file_path': row['File Path'],
                        'faiss_similarity': row.get('FAISS Similarity', 0),
                        'vtkjs_modules': row.get('VTK.js Modules', 'N/A'),
                        'rank_in_stage1': len(result['lost_items']) + 1
                    }
                    result['lost_items'].append(lost_item)
                    result['similarity_distribution'].append(row.get('FAISS Similarity', 0))
                    
                    # ç»Ÿè®¡æ¨¡å—ä¿¡æ¯
                    modules_str = row.get('VTK.js Modules', 'N/A')
                    if modules_str and modules_str != 'N/A':
                        modules = [m.strip() for m in str(modules_str).split(',')]
                        for module in modules:
                            result['module_analysis'][module]['count'] += 1
                            result['module_analysis'][module]['avg_similarity'] += row.get('FAISS Similarity', 0)
                            if len(result['module_analysis'][module]['examples']) < 3:
                                result['module_analysis'][module]['examples'].append({
                                    'file': row['File Path'],
                                    'similarity': row.get('FAISS Similarity', 0)
                                })
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
        for module in result['module_analysis']:
            count = result['module_analysis'][module]['count']
            if count > 0:
                result['module_analysis'][module]['avg_similarity'] = \
                    result['module_analysis'][module]['avg_similarity'] / count
        
        self.analysis_results['loss_analysis'] = result
        return result
    
    def print_loss_summary(self):
        """æ‰“å°ä¸¢å¤±ç»“æœçš„æ‘˜è¦"""
        if 'loss_analysis' not in self.analysis_results:
            self.analyze_loss_between_stages()
        
        result = self.analysis_results.get('loss_analysis', {})
        
        print("\n" + "="*80)
        print("ã€ä¸¤é˜¶æ®µæ£€ç´¢å¯¹æ¯”åˆ†æã€‘")
        print("="*80)
        
        total_stage1 = result.get('total_stage1', 0)
        total_stage2 = result.get('total_stage2', 0)
        lost_count = len(result.get('lost_items', []))
        
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"  â€¢ ç¬¬ä¸€é˜¶æ®µï¼ˆåˆç­›ï¼‰æ€»ç»“æœæ•°: {total_stage1}")
        print(f"  â€¢ ç¬¬äºŒé˜¶æ®µï¼ˆé‡æ’åºï¼‰æ€»ç»“æœæ•°: {total_stage2}")
        print(f"  â€¢ ä¸¢å¤±çš„ç»“æœæ•°: {lost_count}")
        if total_stage1 > 0:
            loss_rate = (lost_count / total_stage1) * 100
            print(f"  â€¢ ä¸¢å¤±ç‡: {loss_rate:.2f}%")
        
        print(f"\nğŸ“ˆ æŒ‰æŸ¥è¯¢ç»Ÿè®¡ä¸¢å¤±ç»“æœ:")
        lost_by_query = result.get('lost_count_by_query', {})
        for query_idx in sorted(lost_by_query.keys()):
            count = lost_by_query[query_idx]
            if count > 0:
                print(f"  â€¢ Query {query_idx}: ä¸¢å¤± {count} ä¸ªç»“æœ")
        
        print(f"\nğŸ” ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
        similarities = result.get('similarity_distribution', [])
        if similarities:
            print(f"  â€¢ æœ€å°ç›¸ä¼¼åº¦: {min(similarities):.4f}")
            print(f"  â€¢ æœ€å¤§ç›¸ä¼¼åº¦: {max(similarities):.4f}")
            print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {np.mean(similarities):.4f}")
            print(f"  â€¢ ä¸­ä½æ•°ç›¸ä¼¼åº¦: {np.median(similarities):.4f}")
        
        print(f"\nğŸ§© æŒ‰VTK.jsæ¨¡å—ç»Ÿè®¡ä¸¢å¤±ç»“æœ:")
        module_analysis = result.get('module_analysis', {})
        sorted_modules = sorted(module_analysis.items(), 
                                key=lambda x: x[1]['count'], 
                                reverse=True)
        
        if sorted_modules:
            print(f"  Top 10 æ¨¡å—ä¸¢å¤±æƒ…å†µ:")
            for idx, (module, stats) in enumerate(sorted_modules[:10], 1):
                print(f"  {idx}. {module}")
                print(f"     - ä¸¢å¤±æ¬¡æ•°: {stats['count']}")
                print(f"     - å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.4f}")
        else:
            print("  æš‚æ— æ¨¡å—ä¿¡æ¯")
    
    def print_lost_details(self, top_n: int = 20):
        """æ‰“å°è¯¦ç»†çš„ä¸¢å¤±ç»“æœ"""
        if 'loss_analysis' not in self.analysis_results:
            self.analyze_loss_between_stages()
        
        lost_items = self.analysis_results.get('loss_analysis', {}).get('lost_items', [])
        
        print(f"\n" + "="*80)
        print(f"ã€ä¸¢å¤±ç»“æœè¯¦ç»†åˆ—è¡¨ã€‘(å±•ç¤ºå‰ {min(top_n, len(lost_items))} ä¸ª)")
        print("="*80 + "\n")
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆé™åºï¼‰
        sorted_items = sorted(lost_items, key=lambda x: x['faiss_similarity'], reverse=True)
        
        for idx, item in enumerate(sorted_items[:top_n], 1):
            print(f"[{idx}] ä¸¢å¤±é¡¹ç›®")
            print(f"  â€¢ æ‰€å±æŸ¥è¯¢: Query {item['query_index']}")
            print(f"  â€¢ æŸ¥è¯¢æè¿°: {item['query_description']}")
            print(f"  â€¢ æ–‡ä»¶è·¯å¾„: {item['file_path']}")
            print(f"  â€¢ FAISSç›¸ä¼¼åº¦: {item['faiss_similarity']:.6f}")
            print(f"  â€¢ VTK.jsæ¨¡å—: {item['vtkjs_modules']}")
            print()
    
    def analyze_stage_comparison(self) -> Dict:
        """
        è¿›è¡Œä¸¤é˜¶æ®µçš„è¯¦ç»†å¯¹æ¯”åˆ†æ
        
        Returns:
            Dict: å¯¹æ¯”åˆ†æç»“æœ
        """
        if self.stage1_df is None or self.stage2_df is None:
            return {}
        
        result = {
            'query_count': len(self.stage1_df['Query Index'].unique()),
            'stage1_stats': self._calculate_stage_stats(self.stage1_df),
            'stage2_stats': self._calculate_stage_stats(self.stage2_df),
            'similarity_threshold_analysis': {}
        }
        
        # ç›¸ä¼¼åº¦é˜ˆå€¼åˆ†æ
        stage1_sims = self.stage1_df['FAISS Similarity'].dropna()
        for threshold in [0.5, 0.6, 0.7, 0.8, 0.9]:
            stage1_above = len(stage1_sims[stage1_sims >= threshold])
            stage2_above = len(self.stage2_df['FAISS Similarity'].dropna()[
                self.stage2_df['FAISS Similarity'].dropna() >= threshold])
            
            result['similarity_threshold_analysis'][threshold] = {
                'stage1': stage1_above,
                'stage2': stage2_above,
                'lost': stage1_above - stage2_above
            }
        
        self.analysis_results['stage_comparison'] = result
        return result
    
    def _calculate_stage_stats(self, df: pd.DataFrame) -> Dict:
        """è®¡ç®—å•ä¸ªé˜¶æ®µçš„ç»Ÿè®¡æ•°æ®"""
        sims = df['FAISS Similarity'].dropna()
        return {
            'total_items': len(df),
            'avg_similarity': float(sims.mean()) if len(sims) > 0 else 0,
            'median_similarity': float(sims.median()) if len(sims) > 0 else 0,
            'min_similarity': float(sims.min()) if len(sims) > 0 else 0,
            'max_similarity': float(sims.max()) if len(sims) > 0 else 0,
            'std_similarity': float(sims.std()) if len(sims) > 0 else 0
        }
    
    def print_stage_comparison(self):
        """æ‰“å°é˜¶æ®µå¯¹æ¯”æŠ¥å‘Š"""
        if 'stage_comparison' not in self.analysis_results:
            self.analyze_stage_comparison()
        
        result = self.analysis_results.get('stage_comparison', {})
        
        print(f"\n" + "="*80)
        print("ã€é˜¶æ®µå¯¹æ¯”ç»Ÿè®¡ã€‘")
        print("="*80)
        
        print(f"\nğŸ“‹ æŸ¥è¯¢æ€»æ•°: {result.get('query_count', 0)}")
        
        stage1_stats = result.get('stage1_stats', {})
        stage2_stats = result.get('stage2_stats', {})
        
        print(f"\nğŸ”· ç¬¬ä¸€é˜¶æ®µï¼ˆåˆç­›ï¼‰ç»Ÿè®¡:")
        print(f"  â€¢ æ€»ç»“æœæ•°: {stage1_stats.get('total_items', 0)}")
        print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {stage1_stats.get('avg_similarity', 0):.6f}")
        print(f"  â€¢ ä¸­ä½æ•°ç›¸ä¼¼åº¦: {stage1_stats.get('median_similarity', 0):.6f}")
        print(f"  â€¢ ç›¸ä¼¼åº¦èŒƒå›´: [{stage1_stats.get('min_similarity', 0):.6f}, {stage1_stats.get('max_similarity', 0):.6f}]")
        print(f"  â€¢ æ ‡å‡†å·®: {stage1_stats.get('std_similarity', 0):.6f}")
        
        print(f"\nğŸ”¶ ç¬¬äºŒé˜¶æ®µï¼ˆé‡æ’åºï¼‰ç»Ÿè®¡:")
        print(f"  â€¢ æ€»ç»“æœæ•°: {stage2_stats.get('total_items', 0)}")
        print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {stage2_stats.get('avg_similarity', 0):.6f}")
        print(f"  â€¢ ä¸­ä½æ•°ç›¸ä¼¼åº¦: {stage2_stats.get('median_similarity', 0):.6f}")
        print(f"  â€¢ ç›¸ä¼¼åº¦èŒƒå›´: [{stage2_stats.get('min_similarity', 0):.6f}, {stage2_stats.get('max_similarity', 0):.6f}]")
        print(f"  â€¢ æ ‡å‡†å·®: {stage2_stats.get('std_similarity', 0):.6f}")
        
        print(f"\nğŸ“Š æŒ‰ç›¸ä¼¼åº¦é˜ˆå€¼ç»Ÿè®¡:")
        threshold_analysis = result.get('similarity_threshold_analysis', {})
        print(f"  {'é˜ˆå€¼':<8} {'ç¬¬ä¸€é˜¶æ®µ':<12} {'ç¬¬äºŒé˜¶æ®µ':<12} {'ä¸¢å¤±':<8}")
        print(f"  {'-'*40}")
        for threshold in sorted(threshold_analysis.keys()):
            stats = threshold_analysis[threshold]
            print(f"  {threshold:<8.1f} {stats['stage1']:<12} {stats['stage2']:<12} {stats['lost']:<8}")
    
    def export_analysis_to_excel(self, output_file: str):
        """
        å°†åˆ†æç»“æœå¯¼å‡ºåˆ°Excelæ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if 'loss_analysis' not in self.analysis_results:
            self.analyze_loss_between_stages()
        
        lost_items = self.analysis_results.get('loss_analysis', {}).get('lost_items', [])
        
        # å¦‚æœæ²¡æœ‰ä¸¢å¤±çš„é¡¹ç›®ï¼Œåˆ›å»ºä¸€ä¸ªè¯´æ˜æ–‡ä»¶
        if not lost_items:
            summary_data = {
                'åˆ†æé¡¹': ['æ€»ç»“æœæ•°(åˆç­›)', 'æ€»ç»“æœæ•°(é‡æ’åº)', 'ä¸¢å¤±æ•°', 'ä¸¢å¤±ç‡(%)'],
                'æ•°å€¼': [
                    self.analysis_results.get('loss_analysis', {}).get('total_stage1', 0),
                    self.analysis_results.get('loss_analysis', {}).get('total_stage2', 0),
                    0,
                    0.0
                ]
            }
            df_summary = pd.DataFrame(summary_data)
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                df_summary.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
                # æ·»åŠ ç©ºç™½çš„ä¸¢å¤±ç»“æœsheet
                empty_df = pd.DataFrame(columns=['Note'])
                empty_df.loc[0] = ['åœ¨æœ¬æ¬¡åˆ†æä¸­ï¼Œåˆç­›å’Œé‡æ’åºé˜¶æ®µçš„ç»“æœå®Œå…¨ç›¸åŒï¼Œæ²¡æœ‰ä¸¢å¤±çš„é¡¹ç›®']
                empty_df.to_excel(writer, sheet_name='ä¸¢å¤±ç»“æœ', index=False)
            print(f"âœ“ åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_file}ï¼ˆæ— ä¸¢å¤±é¡¹ç›®ï¼‰")
            return
        
        # åˆ›å»ºDataFrame
        df_lost = pd.DataFrame(lost_items)
        
        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
        column_order = ['query_index', 'query_description', 'file_path', 'faiss_similarity', 
                        'vtkjs_modules', 'rank_in_stage1']
        df_lost = df_lost[[col for col in column_order if col in df_lost.columns]]
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        if 'faiss_similarity' in df_lost.columns:
            df_lost = df_lost.sort_values('faiss_similarity', ascending=False)
        
        # å¯¼å‡ºåˆ°Excel
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            df_lost.to_excel(writer, sheet_name='ä¸¢å¤±ç»“æœ', index=False)
            
            # æ·»åŠ ç»Ÿè®¡æ‘˜è¦Sheet
            summary_data = {
                'æŒ‡æ ‡': [
                    'ç¬¬ä¸€é˜¶æ®µæ€»ç»“æœæ•°',
                    'ç¬¬äºŒé˜¶æ®µæ€»ç»“æœæ•°',
                    'ä¸¢å¤±ç»“æœæ•°',
                    'ä¸¢å¤±ç‡(%)',
                    'å¹³å‡ç›¸ä¼¼åº¦',
                    'æœ€å°ç›¸ä¼¼åº¦',
                    'æœ€å¤§ç›¸ä¼¼åº¦'
                ],
                'æ•°å€¼': [
                    self.analysis_results['loss_analysis'].get('total_stage1', 0),
                    self.analysis_results['loss_analysis'].get('total_stage2', 0),
                    len(lost_items),
                    (len(lost_items) / max(self.analysis_results['loss_analysis'].get('total_stage1', 1), 1)) * 100,
                    np.mean(self.analysis_results['loss_analysis'].get('similarity_distribution', [0])),
                    min(self.analysis_results['loss_analysis'].get('similarity_distribution', [0])),
                    max(self.analysis_results['loss_analysis'].get('similarity_distribution', [0]))
                ]
            }
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
        
        print(f"âœ“ åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
    
    def plot_similarity_distribution(self, output_file: str = None):
        """
        ç»˜åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒå›¾
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨ï¼‰
        """
        if 'loss_analysis' not in self.analysis_results:
            self.analyze_loss_between_stages()
        
        similarities = self.analysis_results.get('loss_analysis', {}).get('similarity_distribution', [])
        
        if not similarities:
            print("æš‚æ— ä¸¢å¤±é¡¹ç›®ï¼Œæ— éœ€ç»˜åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒå›¾")
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # ç›´æ–¹å›¾
        axes[0].hist(similarities, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
        axes[0].set_xlabel('FAISS ç›¸ä¼¼åº¦', fontsize=12)
        axes[0].set_ylabel('é¢‘æ•°', fontsize=12)
        axes[0].set_title('ä¸¢å¤±ç»“æœçš„ç›¸ä¼¼åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[0].grid(True, alpha=0.3)
        
        # ç®±çº¿å›¾
        axes[1].boxplot(similarities, vert=True)
        axes[1].set_ylabel('FAISS ç›¸ä¼¼åº¦', fontsize=12)
        axes[1].set_title('ä¸¢å¤±ç»“æœçš„ç›¸ä¼¼åº¦ç®±çº¿å›¾', fontsize=14, fontweight='bold')
        axes[1].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        if output_file:
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"âœ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
        else:
            plt.show()
    
    def analyze_vtkjs_modules(self) -> Dict:
        """
        åˆ†ææ‰€æœ‰VTK.jsæ¨¡å—çš„ä½¿ç”¨æƒ…å†µ
        
        Returns:
            Dict: æ¨¡å—ä½¿ç”¨ç»Ÿè®¡
        """
        if self.stage1_df is None:
            return {}
        
        result = {
            'total_modules': {},
            'modules_per_result': [],
            'most_common_modules': []
        }
        
        # ç»Ÿè®¡æ‰€æœ‰æ¨¡å—
        all_modules = defaultdict(int)
        
        for _, row in self.stage1_df.iterrows():
            modules_str = row.get('VTK.js Modules', 'N/A')
            if modules_str and modules_str != 'N/A':
                modules = [m.strip() for m in str(modules_str).split(',')]
                result['modules_per_result'].append(len(modules))
                for module in modules:
                    all_modules[module] += 1
        
        # æ’åº
        result['total_modules'] = dict(sorted(all_modules.items(), 
                                             key=lambda x: x[1], 
                                             reverse=True))
        result['most_common_modules'] = list(result['total_modules'].items())[:20]
        
        self.analysis_results['module_analysis'] = result
        return result
    
    def print_module_statistics(self):
        """
        æ‰“å°æ¨¡å—ç»Ÿè®¡ä¿¡æ¯
        """
        if 'module_analysis' not in self.analysis_results:
            self.analyze_vtkjs_modules()
        
        result = self.analysis_results.get('module_analysis', {})
        total_modules = result.get('total_modules', {})
        modules_per_result = result.get('modules_per_result', [])
        
        print(f"\n" + "="*80)
        print("ã€VTK.js æ¨¡å—ç»Ÿè®¡åˆ†æã€‘")
        print("="*80)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  â€¢ ä¸åŒæ¨¡å—æ€»æ•°: {len(total_modules)}")
        if modules_per_result:
            print(f"  â€¢ å¹³å‡æ¯ä¸ªç»“æœåŒ…å«çš„æ¨¡å—æ•°: {np.mean(modules_per_result):.2f}")
            print(f"  â€¢ æœ€å¤šæ¨¡å—æ•°: {max(modules_per_result)}")
            print(f"  â€¢ æœ€å°‘æ¨¡å—æ•°: {min(modules_per_result)}")
        
        print(f"\nğŸ” ä½¿ç”¨æœ€é¢‘ç¹çš„ 15 ä¸ªæ¨¡å—:")
        for idx, (module, count) in enumerate(list(total_modules.items())[:15], 1):
            percentage = (count / sum(total_modules.values())) * 100
            bar_length = int(percentage / 2)
            bar = 'â–ˆ' * bar_length
            print(f"  {idx:2}. {module[:50]:<50} {count:>4} æ¬¡ ({percentage:>5.1f}%) {bar}")
    
    def analyze_query_effectiveness(self) -> Dict:
        """
        åˆ†ææ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢æ•ˆæœ
        
        Returns:
            Dict: æŸ¥è¯¢æ•ˆæœç»Ÿè®¡
        """
        if self.stage1_df is None:
            return {}
        
        result = {
            'query_stats': {},
            'best_queries': [],
            'worst_queries': []
        }
        
        stage1_by_query = self.stage1_df.groupby('Query Index')
        
        for query_idx in stage1_by_query.groups.keys():
            group = stage1_by_query.get_group(query_idx)
            sims = group['FAISS Similarity'].dropna()
            
            stats = {
                'query_index': query_idx,
                'result_count': len(group),
                'avg_similarity': float(sims.mean()) if len(sims) > 0 else 0,
                'max_similarity': float(sims.max()) if len(sims) > 0 else 0,
                'min_similarity': float(sims.min()) if len(sims) > 0 else 0,
                'high_quality_count': len(sims[sims >= 0.5])  # é«˜è´¨é‡ç»“æœ
            }
            result['query_stats'][query_idx] = stats
        
        # æ’åºæ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æŸ¥è¯¢
        sorted_queries = sorted(result['query_stats'].items(), 
                               key=lambda x: x[1]['avg_similarity'], 
                               reverse=True)
        result['best_queries'] = sorted_queries[:5]
        result['worst_queries'] = sorted_queries[-5:]
        
        self.analysis_results['query_effectiveness'] = result
        return result
    
    def print_query_effectiveness(self):
        """
        æ‰“å°æŸ¥è¯¢æ•ˆæœåˆ†æ
        """
        if 'query_effectiveness' not in self.analysis_results:
            self.analyze_query_effectiveness()
        
        result = self.analysis_results.get('query_effectiveness', {})
        best_queries = result.get('best_queries', [])
        worst_queries = result.get('worst_queries', [])
        query_stats = result.get('query_stats', {})
        
        print(f"\n" + "="*80)
        print("ã€æŸ¥è¯¢æ•ˆæœåˆ†æã€‘")
        print("="*80)
        
        print(f"\nğŸ† æ•ˆæœæœ€å¥½çš„ 5 ä¸ªæŸ¥è¯¢:")
        for idx, (query_id, stats) in enumerate(best_queries, 1):
            print(f"  {idx}. Query {query_id}")
            print(f"     - ç»“æœæ•°: {stats['result_count']}")
            print(f"     - å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.6f}")
            print(f"     - ç›¸ä¼¼åº¦èŒƒå›´: [{stats['min_similarity']:.6f}, {stats['max_similarity']:.6f}]")
            print(f"     - é«˜è´¨é‡ç»“æœæ•°(â‰¥0.5): {stats['high_quality_count']}\n")
        
        print(f"\nğŸ“‰ æ•ˆæœæœ€å·®çš„ 5 ä¸ªæŸ¥è¯¢:")
        for idx, (query_id, stats) in enumerate(reversed(worst_queries), 1):
            print(f"  {idx}. Query {query_id}")
            print(f"     - ç»“æœæ•°: {stats['result_count']}")
            print(f"     - å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.6f}")
            print(f"     - ç›¸ä¼¼åº¦èŒƒå›´: [{stats['min_similarity']:.6f}, {stats['max_similarity']:.6f}]")
            print(f"     - é«˜è´¨é‡ç»“æœæ•°(â‰¥0.5): {stats['high_quality_count']}\n")
    
    def analyze_topk_vs_remaining(self, queries_list: List[str], k: int = 4, similarity_threshold: float = 0.1):
        """
        åˆ†æé‡æ’åºåçš„å‰Kä¸ªé€‰ä¸­ç»“æœ vs å‰©ä½™ç»“æœçš„å·®å¼‚ã€‚
        
        Args:
            queries_list: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
            k: å‰Kä¸ªç»“æœçš„æ•°é‡
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            dict: åŒ…å«å¯¹æ¯”åˆ†æçš„æ•°æ®
        """
        from RAG.embedding_v3_1 import search_code_with_topk_analysis
        
        print(f"\nå¼€å§‹åˆ†æå‰Kä¸ªé€‰ä¸­ç»“æœ vs å‰©ä½™ç»“æœçš„å·®å¼‚ (K={k})...\n")
        
        all_analysis = {
            'queries_analyzed': 0,
            'total_topk_count': 0,
            'total_remaining_count': 0,
            'query_analyses': [],
            'aggregate_stats': {
                'topk_avg_faiss_sim': [],
                'remaining_avg_faiss_sim': [],
                'topk_avg_rerank_score': [],
                'remaining_avg_rerank_score': [],
                'similarity_diffs': [],
                'rerank_diffs': []
            }
        }
        
        for query in queries_list:
            if not query or query.strip() == '':
                continue
            
            try:
                result = search_code_with_topk_analysis(query, k, similarity_threshold)
                analysis = result['analysis']
                
                all_analysis['queries_analyzed'] += 1
                all_analysis['total_topk_count'] += analysis['top_k_count']
                all_analysis['total_remaining_count'] += analysis['remaining_count']
                
                # æ”¶é›†èšåˆç»Ÿè®¡
                if analysis['top_k_stats']['count'] > 0:
                    all_analysis['aggregate_stats']['topk_avg_faiss_sim'].append(
                        analysis['top_k_stats']['avg_faiss_similarity']
                    )
                    all_analysis['aggregate_stats']['topk_avg_rerank_score'].append(
                        analysis['top_k_stats']['avg_rerank_score']
                    )
                
                if analysis['remaining_stats']['count'] > 0:
                    all_analysis['aggregate_stats']['remaining_avg_faiss_sim'].append(
                        analysis['remaining_stats']['avg_faiss_similarity']
                    )
                    all_analysis['aggregate_stats']['remaining_avg_rerank_score'].append(
                        analysis['remaining_stats']['avg_rerank_score']
                    )
                
                # æ”¶é›†å·®å¼‚æ•°æ®
                all_analysis['aggregate_stats']['similarity_diffs'].append(
                    analysis['comparison']['faiss_similarity_diff']
                )
                all_analysis['aggregate_stats']['rerank_diffs'].append(
                    analysis['comparison']['rerank_score_diff']
                )
                
                # ä¿å­˜å•ä¸ªæŸ¥è¯¢çš„åˆ†æ
                all_analysis['query_analyses'].append(analysis)
                
                # æ‰“å°å•ä¸ªæŸ¥è¯¢çš„ç»“æœ
                print(f"æŸ¥è¯¢: {query}")
                print(f"  åˆç­›ç»“æœæ•°: {analysis['total_raw_count']}")
                print(f"  é‡æ’åºç»“æœæ•°: {analysis['total_reranked_count']}")
                print(f"  å‰Kä¸ªé€‰ä¸­: {analysis['top_k_count']}, å‰©ä½™: {analysis['remaining_count']}")
                print(f"  å‰Kç›¸ä¼¼åº¦: {analysis['top_k_stats']['avg_faiss_similarity']:.4f} (èŒƒå›´: {analysis['top_k_stats']['min_faiss_similarity']:.4f}-{analysis['top_k_stats']['max_faiss_similarity']:.4f})")
                print(f"  å‰©ä½™ç›¸ä¼¼åº¦: {analysis['remaining_stats']['avg_faiss_similarity']:.4f} (èŒƒå›´: {analysis['remaining_stats']['min_faiss_similarity']:.4f}-{analysis['remaining_stats']['max_faiss_similarity']:.4f})")
                print(f"  å‰Ké‡æ’åˆ†æ•°: {analysis['top_k_stats']['avg_rerank_score']:.4f}")
                print(f"  å‰©ä½™é‡æ’åˆ†æ•°: {analysis['remaining_stats']['avg_rerank_score']:.4f}")
                print(f"  ç›¸ä¼¼åº¦å·®å¼‚: {analysis['comparison']['faiss_similarity_diff']:.4f}")
                print(f"  é‡æ’åˆ†æ•°å·®å¼‚: {analysis['comparison']['rerank_score_diff']:.4f}\n")
                
            except Exception as e:
                print(f"  åˆ†ææŸ¥è¯¢å¤±è´¥: {e}")
        
        # è®¡ç®—èšåˆç»Ÿè®¡
        if all_analysis['aggregate_stats']['topk_avg_faiss_sim']:
            avg_vals = all_analysis['aggregate_stats']['topk_avg_faiss_sim']
            all_analysis['aggregate_stats']['topk_avg_faiss_sim_mean'] = sum(avg_vals) / len(avg_vals)
        
        if all_analysis['aggregate_stats']['remaining_avg_faiss_sim']:
            avg_vals = all_analysis['aggregate_stats']['remaining_avg_faiss_sim']
            all_analysis['aggregate_stats']['remaining_avg_faiss_sim_mean'] = sum(avg_vals) / len(avg_vals)
        
        if all_analysis['aggregate_stats']['similarity_diffs']:
            diffs = all_analysis['aggregate_stats']['similarity_diffs']
            all_analysis['aggregate_stats']['avg_similarity_diff'] = sum(diffs) / len(diffs)
        
        if all_analysis['aggregate_stats']['rerank_diffs']:
            diffs = all_analysis['aggregate_stats']['rerank_diffs']
            all_analysis['aggregate_stats']['avg_rerank_diff'] = sum(diffs) / len(diffs)
        
        self.topk_analysis_results = all_analysis
        return all_analysis
    
    def export_topk_analysis_to_excel(self, output_file: str):
        """
        å°†å‰K vs å‰©ä½™ç»“æœçš„åˆ†æå¯¼å‡ºåˆ°Excelã€‚
        """
        if not hasattr(self, 'topk_analysis_results') or not self.topk_analysis_results:
            print("æ²¡æœ‰é¡¶Kåˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ analyze_topk_vs_remaining()")
            return
        
        analysis = self.topk_analysis_results
        
        # åˆ›å»ºèšåˆç»Ÿè®¡Sheet
        summary_data = {
            'æŒ‡æ ‡': [
                'æŸ¥è¯¢æ€»æ•°',
                'å‰Kæ€»æ•°',
                'å‰©ä½™æ€»æ•°',
                'å‰Kå¹³å‡ç›¸ä¼¼åº¦',
                'å‰©ä½™å¹³å‡ç›¸ä¼¼åº¦',
                'å¹³å‡ç›¸ä¼¼åº¦å·®å¼‚',
                'å‰Kå¹³å‡é‡æ’åˆ†æ•°',
                'å‰©ä½™å¹³å‡é‡æ’åˆ†æ•°',
                'å¹³å‡é‡æ’åˆ†æ•°å·®å¼‚'
            ],
            'æ•°å€¼': [
                analysis['queries_analyzed'],
                analysis['total_topk_count'],
                analysis['total_remaining_count'],
                analysis['aggregate_stats'].get('topk_avg_faiss_sim_mean', 0),
                analysis['aggregate_stats'].get('remaining_avg_faiss_sim_mean', 0),
                analysis['aggregate_stats'].get('avg_similarity_diff', 0),
                sum(analysis['aggregate_stats']['topk_avg_rerank_score']) / len(analysis['aggregate_stats']['topk_avg_rerank_score']) if analysis['aggregate_stats']['topk_avg_rerank_score'] else 0,
                sum(analysis['aggregate_stats']['remaining_avg_rerank_score']) / len(analysis['aggregate_stats']['remaining_avg_rerank_score']) if analysis['aggregate_stats']['remaining_avg_rerank_score'] else 0,
                analysis['aggregate_stats'].get('avg_rerank_diff', 0)
            ]
        }
        
        # åˆ›å»ºæŸ¥è¯¢è¯¦æƒ…Sheet
        query_details = []
        for qa in analysis['query_analyses']:
            query_details.append({
                'æŸ¥è¯¢': qa['query'],
                'åˆç­›æ•°': qa['total_raw_count'],
                'é‡æ’æ•°': qa['total_reranked_count'],
                'å‰Kæ•°': qa['top_k_count'],
                'å‰©ä½™æ•°': qa['remaining_count'],
                'å‰K_avgç›¸ä¼¼åº¦': qa['top_k_stats']['avg_faiss_similarity'],
                'å‰K_minç›¸ä¼¼åº¦': qa['top_k_stats']['min_faiss_similarity'],
                'å‰K_maxç›¸ä¼¼åº¦': qa['top_k_stats']['max_faiss_similarity'],
                'å‰©ä½™_avgç›¸ä¼¼åº¦': qa['remaining_stats']['avg_faiss_similarity'],
                'å‰©ä½™_minç›¸ä¼¼åº¦': qa['remaining_stats']['min_faiss_similarity'],
                'å‰©ä½™_maxç›¸ä¼¼åº¦': qa['remaining_stats']['max_faiss_similarity'],
                'å‰K_avgé‡æ’åˆ†': qa['top_k_stats']['avg_rerank_score'],
                'å‰©ä½™_avgé‡æ’åˆ†': qa['remaining_stats']['avg_rerank_score'],
                'ç›¸ä¼¼åº¦å·®å¼‚': qa['comparison']['faiss_similarity_diff'],
                'é‡æ’åˆ†å·®å¼‚': qa['comparison']['rerank_score_diff']
            })
        
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # å†™å…¥èšåˆç»Ÿè®¡
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='èšåˆç»Ÿè®¡', index=False)
                
                # å†™å…¥æŸ¥è¯¢è¯¦æƒ…
                df_details = pd.DataFrame(query_details)
                df_details.to_excel(writer, sheet_name='æŸ¥è¯¢è¯¦æƒ…', index=False)
            
            print(f"\nâœ“ é¡¶Kåˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            print(f"âœ— å¯¼å‡ºExcelå¤±è´¥: {e}")
    
    def generate_comprehensive_report(self, output_file: str):
        """
        ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # æ‰§è¡Œæ‰€æœ‰åˆ†æ
        self.analyze_loss_between_stages()
        self.analyze_stage_comparison()
        self.analyze_vtkjs_modules()
        self.analyze_query_effectiveness()
        
        # åˆ›å»ºå¤šä¸ªSheetçš„ExcelæŠ¥å‘Š
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # Sheet 1: ç»Ÿè®¡æ‘˜è¦
            summary_data = {
                'æŒ‡æ ‡': [
                    'ç¬¬ä¸€é˜¶æ®µæ€»ç»“æœæ•°',
                    'ç¬¬äºŒé˜¶æ®µæ€»ç»“æœæ•°',
                    'ä¸¢å¤±ç»“æœæ•°',
                    'ä¸¢å¤±ç‡(%)',
                    'å¹³å‡ç›¸ä¼¼åº¦(ç¬¬ä¸€é˜¶æ®µ)',
                    'ä¸­ä½æ•°ç›¸ä¼¼åº¦(ç¬¬ä¸€é˜¶æ®µ)',
                    'æ ‡å‡†å·®(ç¬¬ä¸€é˜¶æ®µ)',
                    'ä¸åŒVTK.jsæ¨¡å—æ€»æ•°',
                    'å¹³å‡æ¯ç»“æœæ¨¡å—æ•°',
                    'æ€»æŸ¥è¯¢æ•°'
                ],
                'æ•°å€¼': [
                    self.analysis_results.get('loss_analysis', {}).get('total_stage1', 0),
                    self.analysis_results.get('loss_analysis', {}).get('total_stage2', 0),
                    len(self.analysis_results.get('loss_analysis', {}).get('lost_items', [])),
                    (len(self.analysis_results.get('loss_analysis', {}).get('lost_items', [])) / 
                     max(self.analysis_results.get('loss_analysis', {}).get('total_stage1', 1), 1)) * 100,
                    self.analysis_results.get('stage_comparison', {}).get('stage1_stats', {}).get('avg_similarity', 0),
                    self.analysis_results.get('stage_comparison', {}).get('stage1_stats', {}).get('median_similarity', 0),
                    self.analysis_results.get('stage_comparison', {}).get('stage1_stats', {}).get('std_similarity', 0),
                    len(self.analysis_results.get('module_analysis', {}).get('total_modules', {})),
                    np.mean(self.analysis_results.get('module_analysis', {}).get('modules_per_result', [0])),
                    len(self.analysis_results.get('query_effectiveness', {}).get('query_stats', {}))
                ]
            }
            df_summary = pd.DataFrame(summary_data)
            df_summary.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
            
            # Sheet 2: æŒ‰ç›¸ä¼¼åº¦é˜ˆå€¼ç»Ÿè®¡
            threshold_data = []
            for threshold, stats in self.analysis_results.get('stage_comparison', {}).get('similarity_threshold_analysis', {}).items():
                threshold_data.append({
                    'ç›¸ä¼¼åº¦é˜ˆå€¼': threshold,
                    'ç¬¬ä¸€é˜¶æ®µç»“æœæ•°': stats['stage1'],
                    'ç¬¬äºŒé˜¶æ®µç»“æœæ•°': stats['stage2'],
                    'ä¸¢å¤±æ•°': stats['lost']
                })
            if threshold_data:
                df_threshold = pd.DataFrame(threshold_data)
                df_threshold.to_excel(writer, sheet_name='é˜ˆå€¼åˆ†æ', index=False)
            
            # Sheet 3: VTK.js æ¨¡å—ç»Ÿè®¡
            module_data = []
            for module, count in list(self.analysis_results.get('module_analysis', {}).get('total_modules', {}).items())[:50]:
                module_data.append({
                    'æ¨¡å—åç§°': module,
                    'ä½¿ç”¨æ¬¡æ•°': count,
                    'ä½¿ç”¨æ¯”ä¾‹(%)': (count / sum(self.analysis_results.get('module_analysis', {}).get('total_modules', {}).values())) * 100
                })
            if module_data:
                df_modules = pd.DataFrame(module_data)
                df_modules.to_excel(writer, sheet_name='æ¨¡å—ç»Ÿè®¡', index=False)
            
            # Sheet 4: æŸ¥è¯¢æ•ˆæœåˆ†æ
            query_data = []
            for query_id, stats in self.analysis_results.get('query_effectiveness', {}).get('query_stats', {}).items():
                query_data.append({
                    'æŸ¥è¯¢ID': query_id,
                    'ç»“æœæ•°': stats['result_count'],
                    'å¹³å‡ç›¸ä¼¼åº¦': stats['avg_similarity'],
                    'æœ€å¤§ç›¸ä¼¼åº¦': stats['max_similarity'],
                    'æœ€å°ç›¸ä¼¼åº¦': stats['min_similarity'],
                    'é«˜è´¨é‡ç»“æœæ•°': stats['high_quality_count']
                })
            if query_data:
                df_queries = pd.DataFrame(query_data)
                df_queries.to_excel(writer, sheet_name='æŸ¥è¯¢æ•ˆæœ', index=False)
        
        print(f"âœ“ ç»¼åˆæŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")
        """
        ç»˜åˆ¶æ¨¡å—ä¸¢å¤±åˆ†æå›¾
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            top_n: æ˜¾ç¤ºå‰Nä¸ªæ¨¡å—
        """
        if 'loss_analysis' not in self.analysis_results:
            self.analyze_loss_between_stages()
        
        module_analysis = self.analysis_results.get('loss_analysis', {}).get('module_analysis', {})
        
        if not module_analysis:
            print("æš‚æ— æ¨¡å—æ•°æ®ï¼Œæ— éœ€ç»˜åˆ¶æ¨¡å—åˆ†æå›¾")
            return
        
        # æ’åº
        sorted_modules = sorted(module_analysis.items(), 
                                key=lambda x: x[1]['count'], 
                                reverse=True)[:top_n]
        
        modules = [m[0] for m in sorted_modules]
        counts = [m[1]['count'] for m in sorted_modules]
        similarities = [m[1]['avg_similarity'] for m in sorted_modules]
        
        fig, ax1 = plt.subplots(figsize=(14, 8))
        
        # æŸ±çŠ¶å›¾ï¼šä¸¢å¤±æ¬¡æ•°
        x = np.arange(len(modules))
        bars = ax1.bar(x, counts, color='coral', alpha=0.7, label='ä¸¢å¤±æ¬¡æ•°')
        ax1.set_xlabel('VTK.js æ¨¡å—', fontsize=12, fontweight='bold')
        ax1.set_ylabel('ä¸¢å¤±æ¬¡æ•°', fontsize=12, fontweight='bold', color='coral')
        ax1.tick_params(axis='y', labelcolor='coral')
        ax1.set_xticks(x)
        ax1.set_xticklabels(modules, rotation=45, ha='right', fontsize=10)
        
        # åŒYè½´ï¼šå¹³å‡ç›¸ä¼¼åº¦
        ax2 = ax1.twinx()
        line = ax2.plot(x, similarities, color='steelblue', marker='o', linewidth=2, 
                       markersize=8, label='å¹³å‡ç›¸ä¼¼åº¦')
        ax2.set_ylabel('å¹³å‡ç›¸ä¼¼åº¦', fontsize=12, fontweight='bold', color='steelblue')
        ax2.tick_params(axis='y', labelcolor='steelblue')
        
        plt.title('VTK.js æ¨¡å—ä¸¢å¤±åˆ†æï¼ˆä¸¢å¤±æ¬¡æ•° vs å¹³å‡ç›¸ä¼¼åº¦ï¼‰', 
                 fontsize=14, fontweight='bold', pad=20)
        
        # æ·»åŠ å›¾ä¾‹
        bars_label = [bars]
        lines_label = line
        all_labels = [b.get_label() for b in bars_label] + [l.get_label() for l in lines_label]
        ax1.legend(bars_label + lines_label, all_labels, loc='upper right')
        
        plt.tight_layout()
        
        if output_file:
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            print(f"âœ“ æ¨¡å—åˆ†æå›¾å·²ä¿å­˜åˆ°: {output_file}")
        else:
            plt.show()


def main():
    """ä¸»å‡½æ•°"""
    import os
    
    # å·¥ä½œç›®å½•
    work_dir = Path(__file__).parent.parent.parent
    os.chdir(work_dir)
    
    # æ–‡ä»¶è·¯å¾„
    json_file = "retrieval_results_12_2.json"
    excel_file = "retrieval_results_detailed_12_2_15.xlsx"
    
    print("ğŸš€ å¼€å§‹åˆ†ææ£€ç´¢ç»“æœ...\n")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"JSONæ–‡ä»¶: {json_file} - å­˜åœ¨: {Path(json_file).exists()}")
    print(f"Excelæ–‡ä»¶: {excel_file} - å­˜åœ¨: {Path(excel_file).exists()}\n")
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = RetrieverAnalyzer(json_file=json_file, excel_file=excel_file)
    
    # æ‰§è¡Œåˆ†æ
    print("\n" + "="*80)
    analyzer.print_loss_summary()
    analyzer.print_stage_comparison()
    analyzer.print_module_statistics()
    analyzer.print_query_effectiveness()
    
    # å¯¼å‡ºç»“æœ
    output_excel = "experiment_results/analys/retrieval_comprehensive_analysis.xlsx"
    print(f"\næ­£åœ¨ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š: {output_excel}")
    analyzer.generate_comprehensive_report(output_excel)
    
    # å¯¼å‡ºä¸¢å¤±é¡¹ç›®åˆ†æ
    output_excel_lost = "experiment_results/analys/lost_items_detailed_analysis.xlsx"
    print(f"\næ­£åœ¨å¯¼å‡ºä¸¢å¤±é¡¹ç›®åˆ†æ: {output_excel_lost}")
    analyzer.export_analysis_to_excel(output_excel_lost)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")


if __name__ == '__main__':
    main()
